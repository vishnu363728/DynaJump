<task_generation>
<role>
You are an expert robotics engineer specializing in Isaac Lab manipulation tasks. Your expertise includes:
- Robotic manipulation and control systems
- Reward function design for reinforcement learning
- Physics simulation and environment setup
- Isaac Lab framework and conventions
- Sequential task decomposition and sub-problem solving
</role>

<task_description>
pick up a ball
</task_description>

<system_constraints>
<robot>
- Type: Franka Panda arm with parallel gripper
- Reach: Maximum 0.4m radius from base position
- Control: Position and force control capabilities
</robot>

<environment>
- Base: Always includes a table surface
- Objects: Must be positioned within robot reach
- Physics: Realistic object interactions and constraints
- Coordinate system: Positions given as center of mass
</environment>

<framework>
- Platform: Isaac Lab simulation environment
- Import restrictions: Use only imports provided in template files
- Naming convention: snake_case for task names
- Structure: Follow Isaac Lab task organization patterns
</framework>
</system_constraints>

<analysis_framework>
<step1_scene_analysis>
- Identify required objects and their properties
- Determine optimal object placement and orientations
- Consider environmental constraints and physics
- Plan initial scene configuration
</step1_scene_analysis>

<step2_task_decomposition>
- Break task into sequential sub-problems
- Identify key manipulation primitives needed
- Define logical progression through task phases
- Consider failure modes and recovery strategies
</step2_task_decomposition>

<step3_reward_design>
- Design dense reward signals for each sub-problem
- Create smooth reward transitions between phases
- Balance exploration vs exploitation incentives
- Include progress tracking and success metrics
</step3_reward_design>

<step4_observation_space>
- Determine critical state information needed
- Include object poses, robot state, and task progress
- Consider sensor limitations and noise
- Optimize for learning efficiency
</step4_observation_space>

<step5_termination_criteria>
- Define clear success conditions
- Set reasonable failure conditions
- Include timeout and safety terminations
- Handle edge cases and invalid states
</step5_termination_criteria>
</analysis_framework>

<examples>
<example_simple>
<description>pick up a ball</description>
<expansion>
<scene>table + spherical ball object</scene>
<sub_problems>
1. approach_ball: Move end-effector near ball
2. align_gripper: Orient gripper for optimal grasp
3. close_gripper: Execute grasp with appropriate force
4. lift_ball: Raise ball above table surface
</sub_problems>
<rewards>
- Distance-based reward for approaching ball
- Orientation reward for gripper alignment
- Grasp success reward for secure grip
- Height reward for successful lift
</rewards>
</expansion>
</example_simple>

<example_complex>
<description>move block from top drawer to bottom drawer</description>
<expansion>
<scene>cabinet with two drawers + block object in top drawer</scene>
<sub_problems>
1. open_top_drawer: Pull handle to access block
2. grasp_block: Secure grip on block
3. lift_block: Remove block from drawer
4. close_top_drawer: Return drawer to closed position
5. open_bottom_drawer: Access destination location
6. place_block: Position block in bottom drawer
7. close_bottom_drawer: Complete task
</sub_problems>
<rewards>
- Drawer opening progress rewards
- Grasp success and stability rewards
- Block positioning and placement rewards
- Task completion and efficiency rewards
</rewards>
</expansion>
</example_complex>
</examples>

<code_generation_requirements>
<completeness>
- Generate complete, executable code for ALL files
- No placeholders, comments, or partial implementations
- All template variables must be replaced with task-specific values
- Include proper error handling and edge case management
</completeness>

<quality_standards>
- Follow Isaac Lab coding conventions and style
- Use descriptive variable names and clear comments
- Implement robust reward functions with proper scaling
- Include comprehensive observation and action spaces
- Set appropriate episode lengths for task complexity
</quality_standards>

<file_specifications>
- __init__.py: Proper module imports and exports
- env_cfg.py: Complete environment configuration
- observations.py: Comprehensive state observation functions
- rewards.py: Dense, shaped reward function implementations
- terminations.py: Success, failure, and timeout conditions
- joint_pos_env_cfg.py: Robot-specific configuration
- rsl_rl_ppo_cfg.py: Training algorithm parameters
</file_specifications>
</code_generation_requirements>

<template_files>
The following template files provide the structure you must populate:

<template_file>
<path>__init__.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""Configurations for the object lift environments."""

# We leave this file empty since we don't want to expose any configs in this package directly.
# We still need this file to import the "config" module in the parent package.

</content>
</template_file>

<template_file>
<path>task_name/task_name_env_cfg.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

from dataclasses import MISSING

import isaaclab.sim as sim_utils
from isaaclab.actuators.actuator_cfg import ImplicitActuatorCfg
from isaaclab.assets import ArticulationCfg, AssetBaseCfg, DeformableObjectCfg, RigidObjectCfg
from isaaclab.envs import ManagerBasedRLEnvCfg
from isaaclab.managers import CurriculumTermCfg as CurrTerm
from isaaclab.managers import EventTermCfg as EventTerm
from isaaclab.managers import ObservationGroupCfg as ObsGroup
from isaaclab.managers import ObservationTermCfg as ObsTerm
from isaaclab.managers import RewardTermCfg as RewTerm
from isaaclab.managers import SceneEntityCfg
from isaaclab.managers import TerminationTermCfg as DoneTerm
from isaaclab.scene import InteractiveSceneCfg
from isaaclab.sensors import FrameTransformerCfg
from isaaclab.sensors.frame_transformer import OffsetCfg
from isaaclab.sim.spawners.from_files.from_files_cfg import GroundPlaneCfg, UsdFileCfg
from isaaclab.utils import configclass
from isaaclab.utils.assets import ISAAC_NUCLEUS_DIR

from . import mdp

##
# Pre-defined configs
##
from isaaclab.markers.config import FRAME_MARKER_CFG  # isort: skip

FRAME_MARKER_SMALL_CFG = FRAME_MARKER_CFG.copy()
FRAME_MARKER_SMALL_CFG.markers["frame"].scale = (0.10, 0.10, 0.10)


##
# Scene definition
##

@configclass
class {TASK_NAME_PASCAL}SceneCfg(InteractiveSceneCfg):
    """Configuration for the {TASK_NAME} scene with a robot and task-specific objects.
    
    This is the abstract base implementation, the exact scene is defined in the derived classes
    which need to set the robot and end-effector frames.
    """

    # robots: will be populated by agent env cfg
    robot: ArticulationCfg = MISSING
    # end-effector sensor: will be populated by agent env cfg
    ee_frame: FrameTransformerCfg = MISSING
    
    # TODO: Add task-specific objects below
    # Choose appropriate object types based on your task requirements

    # Example: Main interaction object (customize based on task)
    {PRIMARY_OBJECT}: RigidObjectCfg | ArticulationCfg = MISSING  # TODO: Define objects, but keep them equal to MISSING here, as it will be populated in joint_pos_env file
    # TODO: Add additional objects as needed
    # Example patterns:
    # DO NOT USE USD FILES AT ALL FOR OBJECTS!!!, if you don't see an example for an object, please construct it to the best of your ability
    # - Fixed objects: Use RigidObjectCfg for simple objects
    # - Articulated objects: Use ArticulationCfg for objects with joints (doors, drawers, etc.)
    # - Deformable objects: Use DeformableObjectCfg for soft objects
    
    # Table (if needed for manipulation tasks)
    table = AssetBaseCfg(
        prim_path="{ENV_REGEX_NS}/Table",
        init_state=AssetBaseCfg.InitialStateCfg(pos=[0.5, 0, 0], rot=[0.707, 0, 0, 0.707]),
        spawn=UsdFileCfg(usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Mounts/SeattleLabTable/table_instanceable.usd"),
    )

    # TODO: Add frame transformers for interaction points
    # Example for articulated objects with handles/interaction points:
    # {OBJECT_NAME}_frame = FrameTransformerCfg(
    #     prim_path="{ENV_REGEX_NS}/{OBJECT_NAME}/{BASE_LINK}",
    #     debug_vis=True,
    #     visualizer_cfg=FRAME_MARKER_SMALL_CFG.replace(prim_path="/Visuals/{OBJECT_NAME}FrameTransformer"),
    #     target_frames=[
    #         FrameTransformerCfg.FrameCfg(
    #             prim_path="{ENV_REGEX_NS}/{OBJECT_NAME}/{INTERACTION_LINK}",
    #             name="{INTERACTION_POINT_NAME}",
    #             offset=OffsetCfg(
    #                 pos=({OFFSET_X}, {OFFSET_Y}, {OFFSET_Z}),
    #                 rot=({ROT_W}, {ROT_X}, {ROT_Y}, {ROT_Z}),  # align with end-effector frame
    #             ),
    #         ),
    #     ],
    # )

    # Ground plane
    plane = AssetBaseCfg(
        prim_path="/World/GroundPlane",
        init_state=AssetBaseCfg.InitialStateCfg(pos=[0, 0, -1.05]),
        spawn=GroundPlaneCfg(),
    )

    # Lighting
    light = AssetBaseCfg(
        prim_path="/World/light",
        spawn=sim_utils.DomeLightCfg(color=(0.75, 0.75, 0.75), intensity=3000.0),
    )


##
# MDP settings
##

@configclass
class CommandsCfg:
    """Command terms for the MDP."""
    
    # TODO: Add commands if your task requires goal-directed behavior
    # Example for pose commands:
    # {COMMAND_NAME} = mdp.UniformPoseCommandCfg(
    #     asset_name="robot",
    #     body_name=MISSING,  # will be set by agent env cfg
    #     resampling_time_range=(5.0, 5.0),
    #     debug_vis=True,
    #     ranges=mdp.UniformPoseCommandCfg.Ranges(
    #         pos_x=({MIN_X}, {MAX_X}), pos_y=({MIN_Y}, {MAX_Y}), pos_z=({MIN_Z}, {MAX_Z}),
    #         roll=(0.0, 0.0), pitch=(0.0, 0.0), yaw=(0.0, 0.0)
    #     ),
    # )
    pass  # Remove this if adding commands


@configclass
class ActionsCfg:
    """Action specifications for the MDP."""

    # Will be set by agent env cfg
    arm_action: mdp.JointPositionActionCfg | mdp.DifferentialInverseKinematicsActionCfg = MISSING
    gripper_action: mdp.BinaryJointPositionActionCfg = MISSING


@configclass
class ObservationsCfg:
    """Observation specifications for the MDP."""

    @configclass
    class PolicyCfg(ObsGroup):
        """Observations for policy group."""

        # Standard robot observations
        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
        actions = ObsTerm(func=mdp.last_action)

        # TODO: Add task-specific observations
        # Common patterns from examples:
        
        # End-effector observations
        # eef_pos = ObsTerm(func=mdp.ee_frame_pos)
        # eef_quat = ObsTerm(func=mdp.ee_frame_quat)
        # gripper_pos = ObsTerm(func=mdp.gripper_pos)

        # Object observations
        # {OBJECT_NAME}_position = ObsTerm(func=mdp.object_position_in_robot_root_frame)
        # {OBJECT_NAME}_orientation = ObsTerm(func=mdp.{OBJECT_NAME}_orientations_in_world_frame)

        # Distance observations
        # rel_ee_{OBJECT_NAME}_distance = ObsTerm(func=mdp.rel_ee_{OBJECT_NAME}_distance)

        # Joint observations for articulated objects
        # {ARTICULATED_OBJECT}_joint_pos = ObsTerm(
        #     func=mdp.joint_pos_rel,
        #     params={"asset_cfg": SceneEntityCfg("{ARTICULATED_OBJECT}", joint_names=["{JOINT_NAME}"])},
        # )
        # {ARTICULATED_OBJECT}_joint_vel = ObsTerm(
        #     func=mdp.joint_vel_rel,
        #     params={"asset_cfg": SceneEntityCfg("{ARTICULATED_OBJECT}", joint_names=["{JOINT_NAME}"])},
        # )

        # Command observations (if using commands)
        # target_{OBJECT_NAME}_position = ObsTerm(func=mdp.generated_commands, params={"command_name": "{COMMAND_NAME}"})

        def __post_init__(self):
            self.enable_corruption = True
            self.concatenate_terms = True

    # TODO: Add additional observation groups if needed
    # Example for multi-modal observations:
    # @configclass
    # class RGBCameraPolicyCfg(ObsGroup):
    #     """Observations for policy group with RGB images."""
    #     
    #     def __post_init__(self):
    #         self.enable_corruption = False
    #         self.concatenate_terms = False

    # @configclass
    # class SubtaskCfg(ObsGroup):
    #     """Observations for subtask completion."""
    #     
    #     {SUBTASK_1} = ObsTerm(
    #         func=mdp.{SUBTASK_1_FUNCTION},
    #         params={
    #             "robot_cfg": SceneEntityCfg("robot"),
    #             "object_cfg": SceneEntityCfg("{OBJECT_NAME}"),
    #         },
    #     )
    #     
    #     def __post_init__(self):
    #         self.enable_corruption = False
    #         self.concatenate_terms = False

    # observation groups
    policy: PolicyCfg = PolicyCfg()


@configclass
class EventCfg:
    """Configuration for events."""

    # Standard reset events
    reset_all = EventTerm(func=mdp.reset_scene_to_default, mode="reset")

    reset_robot_joints = EventTerm(
        func=mdp.reset_joints_by_offset,
        mode="reset",
        params={
            "position_range": (-0.1, 0.1),
            "velocity_range": (0.0, 0.0),
        },
    )

    # TODO: Add task-specific reset events
    # Example for object position randomization:
    # reset_{OBJECT_NAME}_position = EventTerm(
    #     func=mdp.reset_root_state_uniform,
    #     mode="reset",
    #     params={
    #         "pose_range": {"x": ({MIN_X}, {MAX_X}), "y": ({MIN_Y}, {MAX_Y}), "z": ({MIN_Z}, {MAX_Z})},
    #         "velocity_range": {},
    #         "asset_cfg": SceneEntityCfg("{OBJECT_NAME}", body_names="{BODY_NAME}"),
    #     },
    # )

    # TODO: Add domain randomization events
    # Example for physics material randomization:
    # robot_physics_material = EventTerm(
    #     func=mdp.randomize_rigid_body_material,
    #     mode="startup",
    #     params={
    #         "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
    #         "static_friction_range": (0.8, 1.25),
    #         "dynamic_friction_range": (0.8, 1.25),
    #         "restitution_range": (0.0, 0.0),
    #         "num_buckets": 16,
    #     },
    # )


@configclass
class RewardsCfg:
    """Reward terms for the MDP."""

    # TODO: Add task-specific reward terms
    # Use the reward function patterns from the reward functions template
    
    # Example approach rewards:
    # approach_ee_{TARGET} = RewTerm(func=mdp.approach_ee_{TARGET}, weight=2.0, params={"threshold": 0.2})
    
    # Example manipulation rewards:
    # {MANIPULATION_REWARD} = RewTerm(
    #     func=mdp.{MANIPULATION_FUNCTION},
    #     weight=5.0,
    #     params={"asset_cfg": SceneEntityCfg("{TARGET_ASSET}", joint_names=["{JOINT_NAME}"])},
    # )

    # Example distance rewards:
    # reaching_{OBJECT} = RewTerm(func=mdp.object_ee_distance, params={"std": 0.1}, weight=1.0)

    # Example goal tracking rewards (if using commands):
    # {OBJECT}_goal_tracking = RewTerm(
    #     func=mdp.object_goal_distance,
    #     params={"std": 0.3, "minimal_height": 0.04, "command_name": "{COMMAND_NAME}"},
    #     weight=16.0,
    # )

    # Standard penalty terms
    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-1e-4)
    joint_vel = RewTerm(func=mdp.joint_vel_l2, weight=-1e-4)


@configclass
class TerminationsCfg:
    """Termination terms for the MDP."""

    # Standard terminations
    time_out = DoneTerm(func=mdp.time_out, time_out=True)

    # TODO: Add task-specific termination conditions
    # Use the termination function patterns from the termination functions template

    # Example failure terminations:
    # {OBJECT}_dropping = DoneTerm(
    #     func=mdp.root_height_below_minimum,
    #     params={"minimum_height": -0.05, "asset_cfg": SceneEntityCfg("{OBJECT_NAME}")},
    # )

    # Example success terminations:
    # success = DoneTerm(func=mdp.{SUCCESS_FUNCTION})


@configclass
class CurriculumCfg:
    """Curriculum terms for the MDP."""

    # TODO: Add curriculum terms if needed for progressive training
    # Example for gradually increasing penalty weights:
    # action_rate = CurrTerm(
    #     func=mdp.modify_reward_weight,
    #     params={"term_name": "action_rate", "weight": -1e-1, "num_steps": 10000}
    # )


##
# Environment configuration
##

@configclass
class {TASK_NAME_PASCAL}EnvCfg(ManagerBasedRLEnvCfg):
    """Configuration for the {TASK_NAME} environment."""

    # Scene settings
    scene: {TASK_NAME_PASCAL}SceneCfg = {TASK_NAME_PASCAL}SceneCfg(num_envs=4096, env_spacing=2.5)
    
    # Basic settings
    observations: ObservationsCfg = ObservationsCfg()
    actions: ActionsCfg = ActionsCfg()
    
    # MDP settings (add/remove based on your task needs)
    rewards: RewardsCfg = RewardsCfg()
    terminations: TerminationsCfg = TerminationsCfg()
    events: EventCfg = EventCfg()
    
    # TODO: Uncomment if using commands
    # commands: CommandsCfg = CommandsCfg()
    
    # TODO: Uncomment if using curriculum
    # curriculum: CurriculumCfg = CurriculumCfg()

    def __post_init__(self):
        """Post initialization."""
        # TODO: Adjust these settings based on your task requirements
        
        # General settings
        self.decimation = 2  # Control frequency divider
        self.episode_length_s = 5.0  # Episode length in seconds
        
        # Viewer settings (for visualization)
        self.viewer.eye = (-2.0, 2.0, 2.0)  # Camera position
        self.viewer.lookat = (0.0, 0.0, 0.5)  # Camera target
        
        # Simulation settings
        self.sim.dt = 0.01  # 100Hz physics simulation
        self.sim.render_interval = self.decimation  # Rendering frequency
        
        # Physics settings (usually don't need to change these)
        self.sim.physx.bounce_threshold_velocity = 0.2
        self.sim.physx.bounce_threshold_velocity = 0.01
        self.sim.physx.gpu_found_lost_aggregate_pairs_capacity = 1024 * 1024 * 4
        self.sim.physx.gpu_total_aggregate_pairs_capacity = 16 * 1024
        self.sim.physx.friction_correlation_distance = 0.00625


# TODO: Configuration Instructions for LLM:
# 
# When populating this template, replace the following placeholders:
# 
# TASK NAMING:
# - {TASK_NAME}: Task name in lowercase (e.g., "cabinet", "pick_and_place", "door_opening")
# - {TASK_NAME_PASCAL}: Task name in PascalCase (e.g., "Cabinet", "PickAndPlace", "DoorOpening")
# 
# OBJECT CONFIGURATION:
# - {PRIMARY_OBJECT}: Main interaction object name (e.g., "object", "cabinet", "door")
# - {OBJECT_NAME}: Generic object name for templates
# - {ARTICULATED_OBJECT}: Name of articulated objects with joints
# - {TARGET_ASSET}: Asset being manipulated
# 
# INTERACTION POINTS:
# - {INTERACTION_LINK}: Link name for interaction points (e.g., "handle", "button")
# - {INTERACTION_POINT_NAME}: Name for the interaction frame
# - {BASE_LINK}: Base link of articulated objects
# 
# COORDINATE OFFSETS:
# - {OFFSET_X}, {OFFSET_Y}, {OFFSET_Z}: Position offsets for frame transformers
# - {ROT_W}, {ROT_X}, {ROT_Y}, {ROT_Z}: Rotation quaternion for frame alignment
# 
# JOINT CONFIGURATION:
# - {JOINT_NAME}: Specific joint names for articulated objects
# - {JOINT_INDEX}: Joint indices for observations
# 
# COMMAND CONFIGURATION:
# - {COMMAND_NAME}: Name of command terms
# - {MIN_X}, {MAX_X}, etc.: Range limits for commands and randomization
# 
# FUNCTION NAMES:
# - {SUBTASK_1_FUNCTION}: Function names for subtask observations
# - {MANIPULATION_FUNCTION}: Function names for manipulation rewards
# - {SUCCESS_FUNCTION}: Function name for success termination
# 
# CONFIGURATION PATTERNS TO FOLLOW:
# 
# 1. **Simple Object Manipulation** (like lift task):
#    - Use RigidObjectCfg for objects
#    - Add object position/orientation observations
#    - Include distance-based rewards
#    - Add command-based goal tracking
# 
# 2. **Articulated Object Manipulation** (like cabinet task):
#    - Use ArticulationCfg for objects with joints
#    - Add frame transformers for interaction points
#    - Include joint position/velocity observations
#    - Add approach, grasp, and manipulation rewards
# 
# 3. **Multi-Object Tasks** (like stacking):
#    - Multiple object configurations
#    - Subtask observation groups
#    - Complex success termination conditions
#    - Multi-stage reward structures
# 
# STEPS TO CUSTOMIZE:
# 1. Choose appropriate scene objects and their types
# 2. Define interaction points and frame transformers
# 3. Add relevant observations for your task
# 4. Configure appropriate reward structure
# 5. Set up termination conditions
# 6. Add domain randomization events as needed
# 7. Adjust simulation parameters for your task requirements
# 8. Remove TODO comments and unused sections
# 9. Test configuration with your robot-specific implementation

'''

ArticulateCfg:
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from dataclasses import MISSING

from isaaclab.actuators import ActuatorBaseCfg
from isaaclab.utils import configclass

from ..asset_base_cfg import AssetBaseCfg
from .articulation import Articulation


@configclass
class ArticulationCfg(AssetBaseCfg):
    """Configuration parameters for an articulation."""

    @configclass
    class InitialStateCfg(AssetBaseCfg.InitialStateCfg):
        """Initial state of the articulation."""

        # root velocity
        lin_vel: tuple[float, float, float] = (0.0, 0.0, 0.0)
        """Linear velocity of the root in simulation world frame. Defaults to (0.0, 0.0, 0.0)."""
        ang_vel: tuple[float, float, float] = (0.0, 0.0, 0.0)
        """Angular velocity of the root in simulation world frame. Defaults to (0.0, 0.0, 0.0)."""

        # joint state
        joint_pos: dict[str, float] = {".*": 0.0}
        """Joint positions of the joints. Defaults to 0.0 for all joints."""
        joint_vel: dict[str, float] = {".*": 0.0}
        """Joint velocities of the joints. Defaults to 0.0 for all joints."""

    ##
    # Initialize configurations.
    ##

    class_type: type = Articulation

    articulation_root_prim_path: str | None = None
    """Path to the articulation root prim in the USD file.

    If not provided will search for a prim with the ArticulationRootAPI. Should start with a slash.
    """

    init_state: InitialStateCfg = InitialStateCfg()
    """Initial state of the articulated object. Defaults to identity pose with zero velocity and zero joint state."""

    soft_joint_pos_limit_factor: float = 1.0
    """Fraction specifying the range of joint position limits (parsed from the asset) to use. Defaults to 1.0.

    The soft joint position limits are scaled by this factor to specify a safety region within the simulated
    joint position limits. This isn't used by the simulation, but is useful for learning agents to prevent the joint
    positions from violating the limits, such as for termination conditions.

    The soft joint position limits are accessible through the :attr:`ArticulationData.soft_joint_pos_limits` attribute.
    """

    actuators: dict[str, ActuatorBaseCfg] = MISSING
    """Actuators for the robot with corresponding joint names."""


AssetBaseCfg:
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from dataclasses import MISSING
from typing import Literal

from isaaclab.sim import SpawnerCfg
from isaaclab.utils import configclass

from .asset_base import AssetBase


@configclass
class AssetBaseCfg:
    """The base configuration class for an asset's parameters.

    Please see the :class:`AssetBase` class for more information on the asset class.
    """

    @configclass
    class InitialStateCfg:
        """Initial state of the asset.

        This defines the default initial state of the asset when it is spawned into the simulation, as
        well as the default state when the simulation is reset.

        After parsing the initial state, the asset class stores this information in the :attr:`data`
        attribute of the asset class. This can then be accessed by the user to modify the state of the asset
        during the simulation, for example, at resets.
        """

        # root position
        pos: tuple[float, float, float] = (0.0, 0.0, 0.0)
        """Position of the root in simulation world frame. Defaults to (0.0, 0.0, 0.0)."""
        rot: tuple[float, float, float, float] = (1.0, 0.0, 0.0, 0.0)
        """Quaternion rotation (w, x, y, z) of the root in simulation world frame.
        Defaults to (1.0, 0.0, 0.0, 0.0).
        """

    class_type: type[AssetBase] = None
    """The associated asset class. Defaults to None, which means that the asset will be spawned
    but cannot be interacted with via the asset class.

    The class should inherit from :class:`isaaclab.assets.asset_base.AssetBase`.
    """

    prim_path: str = MISSING
    """Prim path (or expression) to the asset.

    .. note::
        The expression can contain the environment namespace regex ``{ENV_REGEX_NS}`` which
        will be replaced with the environment namespace.

        Example: ``{ENV_REGEX_NS}/Robot`` will be replaced with ``/World/envs/env_.*/Robot``.
    """

    spawn: SpawnerCfg | None = None
    """Spawn configuration for the asset. Defaults to None.

    If None, then no prims are spawned by the asset class. Instead, it is assumed that the
    asset is already present in the scene.
    """

    init_state: InitialStateCfg = InitialStateCfg()
    """Initial state of the rigid object. Defaults to identity pose."""

    collision_group: Literal[0, -1] = 0
    """Collision group of the asset. Defaults to ``0``.

    * ``-1``: global collision group (collides with all assets in the scene).
    * ``0``: local collision group (collides with other assets in the same environment).
    """

    debug_vis: bool = False
    """Whether to enable debug visualization for the asset. Defaults to ``False``."""


ArticulationCfg



DEFORMABLE OBJECT:


RIGID OBJECT:
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

import torch
from collections.abc import Sequence
from typing import TYPE_CHECKING

import omni.log
import omni.physics.tensors.impl.api as physx
from isaacsim.core.simulation_manager import SimulationManager
from pxr import UsdPhysics

import isaaclab.sim as sim_utils
import isaaclab.utils.math as math_utils
import isaaclab.utils.string as string_utils

from ..asset_base import AssetBase
from .rigid_object_data import RigidObjectData

if TYPE_CHECKING:
    from .rigid_object_cfg import RigidObjectCfg


class RigidObject(AssetBase):
    """A rigid object asset class.

    Rigid objects are assets comprising of rigid bodies. They can be used to represent dynamic objects
    such as boxes, spheres, etc. A rigid body is described by its pose, velocity and mass distribution.

    For an asset to be considered a rigid object, the root prim of the asset must have the `USD RigidBodyAPI`_
    applied to it. This API is used to define the simulation properties of the rigid body. On playing the
    simulation, the physics engine will automatically register the rigid body and create a corresponding
    rigid body handle. This handle can be accessed using the :attr:`root_physx_view` attribute.

    .. note::

        For users familiar with Isaac Sim, the PhysX view class API is not the exactly same as Isaac Sim view
        class API. Similar to Isaac Lab, Isaac Sim wraps around the PhysX view API. However, as of now (2023.1 release),
        we see a large difference in initializing the view classes in Isaac Sim. This is because the view classes
        in Isaac Sim perform additional USD-related operations which are slow and also not required.

    .. _`USD RigidBodyAPI`: https://openusd.org/dev/api/class_usd_physics_rigid_body_a_p_i.html
    """

    cfg: RigidObjectCfg
    """Configuration instance for the rigid object."""

    def __init__(self, cfg: RigidObjectCfg):
        """Initialize the rigid object.

        Args:
            cfg: A configuration instance.
        """
        super().__init__(cfg)

    """
    Properties
    """

    @property
    def data(self) -> RigidObjectData:
        return self._data

    @property
    def num_instances(self) -> int:
        return self.root_physx_view.count

    @property
    def num_bodies(self) -> int:
        """Number of bodies in the asset.

        This is always 1 since each object is a single rigid body.
        """
        return 1

    @property
    def body_names(self) -> list[str]:
        """Ordered names of bodies in the rigid object."""
        prim_paths = self.root_physx_view.prim_paths[: self.num_bodies]
        return [path.split("/")[-1] for path in prim_paths]

    @property
    def root_physx_view(self) -> physx.RigidBodyView:
        """Rigid body view for the asset (PhysX).

        Note:
            Use this view with caution. It requires handling of tensors in a specific way.
        """
        return self._root_physx_view

    """
    Operations.
    """

    def reset(self, env_ids: Sequence[int] | None = None):
        # resolve all indices
        if env_ids is None:
            env_ids = slice(None)
        # reset external wrench
        self._external_force_b[env_ids] = 0.0
        self._external_torque_b[env_ids] = 0.0

    def write_data_to_sim(self):
        """Write external wrench to the simulation.

        Note:
            We write external wrench to the simulation here since this function is called before the simulation step.
            This ensures that the external wrench is applied at every simulation step.
        """
        # write external wrench
        if self.has_external_wrench:
            self.root_physx_view.apply_forces_and_torques_at_position(
                force_data=self._external_force_b.view(-1, 3),
                torque_data=self._external_torque_b.view(-1, 3),
                position_data=None,
                indices=self._ALL_INDICES,
                is_global=False,
            )

    def update(self, dt: float):
        self._data.update(dt)

    """
    Operations - Finders.
    """

    def find_bodies(self, name_keys: str | Sequence[str], preserve_order: bool = False) -> tuple[list[int], list[str]]:
        """Find bodies in the rigid body based on the name keys.

        Please check the :meth:`isaaclab.utils.string_utils.resolve_matching_names` function for more
        information on the name matching.

        Args:
            name_keys: A regular expression or a list of regular expressions to match the body names.
            preserve_order: Whether to preserve the order of the name keys in the output. Defaults to False.

        Returns:
            A tuple of lists containing the body indices and names.
        """
        return string_utils.resolve_matching_names(name_keys, self.body_names, preserve_order)

    """
    Operations - Write to simulation.
    """

    def write_root_state_to_sim(self, root_state: torch.Tensor, env_ids: Sequence[int] | None = None):
        """Set the root state over selected environment indices into the simulation.

        The root state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear
        and angular velocity. All the quantities are in the simulation frame.

        Args:
            root_state: Root state in simulation frame. Shape is (len(env_ids), 13).
            env_ids: Environment indices. If None, then all indices are used.
        """
        self.write_root_link_pose_to_sim(root_state[:, :7], env_ids=env_ids)
        self.write_root_com_velocity_to_sim(root_state[:, 7:], env_ids=env_ids)

    def write_root_com_state_to_sim(self, root_state: torch.Tensor, env_ids: Sequence[int] | None = None):
        """Set the root center of mass state over selected environment indices into the simulation.

        The root state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear
        and angular velocity. All the quantities are in the simulation frame.

        Args:
            root_state: Root state in simulation frame. Shape is (len(env_ids), 13).
            env_ids: Environment indices. If None, then all indices are used.
        """
        self.write_root_com_pose_to_sim(root_state[:, :7], env_ids=env_ids)
        self.write_root_com_velocity_to_sim(root_state[:, 7:], env_ids=env_ids)

    def write_root_link_state_to_sim(self, root_state: torch.Tensor, env_ids: Sequence[int] | None = None):
        """Set the root link state over selected environment indices into the simulation.

        The root state comprises of the cartesian position, quaternion orientation in (w, x, y, z), and linear
        and angular velocity. All the quantities are in the simulation frame.

        Args:
            root_state: Root state in simulation frame. Shape is (len(env_ids), 13).
            env_ids: Environment indices. If None, then all indices are used.
        """
        self.write_root_link_pose_to_sim(root_state[:, :7], env_ids=env_ids)
        self.write_root_link_velocity_to_sim(root_state[:, 7:], env_ids=env_ids)

    def write_root_pose_to_sim(self, root_pose: torch.Tensor, env_ids: Sequence[int] | None = None):
        """Set the root pose over selected environment indices into the simulation.

        The root pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).

        Args:
            root_pose: Root link poses in simulation frame. Shape is (len(env_ids), 7).
            env_ids: Environment indices. If None, then all indices are used.
        """
        self.write_root_link_pose_to_sim(root_pose, env_ids=env_ids)

    def write_root_link_pose_to_sim(self, root_pose: torch.Tensor, env_ids: Sequence[int] | None = None):
        """Set the root link pose over selected environment indices into the simulation.

        The root pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).

        Args:
            root_pose: Root link poses in simulation frame. Shape is (len(env_ids), 7).
            env_ids: Environment indices. If None, then all indices are used.
        """
        # resolve all indices
        physx_env_ids = env_ids
        if env_ids is None:
            env_ids = slice(None)
            physx_env_ids = self._ALL_INDICES

        # note: we need to do this here since tensors are not set into simulation until step.
        # set into internal buffers
        self._data.root_link_pose_w[env_ids] = root_pose.clone()
        # update these buffers only if the user is using them. Otherwise this adds to overhead.
        if self._data._root_link_state_w.data is not None:
            self._data.root_link_state_w[env_ids, :7] = self._data.root_link_pose_w[env_ids]
        if self._data._root_state_w.data is not None:
            self._data.root_state_w[env_ids, :7] = self._data.root_link_pose_w[env_ids]
        if self._data._root_com_state_w.data is not None:
            expected_com_pos, expected_com_quat = math_utils.combine_frame_transforms(
                self._data.root_link_pose_w[env_ids, :3],
                self._data.root_link_pose_w[env_ids, 3:7],
                self.data.body_com_pos_b[env_ids, 0, :],
                self.data.body_com_quat_b[env_ids, 0, :],
            )
            self._data.root_com_state_w[env_ids, :3] = expected_com_pos
            self._data.root_com_state_w[env_ids, 3:7] = expected_com_quat
        # convert root quaternion from wxyz to xyzw
        root_poses_xyzw = self._data.root_link_pose_w.clone()
        root_poses_xyzw[:, 3:] = math_utils.convert_quat(root_poses_xyzw[:, 3:], to="xyzw")
        # set into simulation
        self.root_physx_view.set_transforms(root_poses_xyzw, indices=physx_env_ids)

    def write_root_com_pose_to_sim(self, root_pose: torch.Tensor, env_ids: Sequence[int] | None = None):
        """Set the root center of mass pose over selected environment indices into the simulation.

        The root pose comprises of the cartesian position and quaternion orientation in (w, x, y, z).
        The orientation is the orientation of the principle axes of inertia.

        Args:
            root_pose: Root center of mass poses in simulation frame. Shape is (len(env_ids), 7).
            env_ids: Environment indices. If None, then all indices are used.
        """
        # resolve all indices
        if env_ids is None:
            local_env_ids = slice(env_ids)
        else:
            local_env_ids = env_ids

        # set into internal buffers
        self._data.root_com_pose_w[local_env_ids] = root_pose.clone()
        # update these buffers only if the user is using them. Otherwise this adds to overhead.
        if self._data._root_com_state_w.data is not None:
            self._data.root_com_state_w[local_env_ids, :7] = self._data.root_com_pose_w[local_env_ids]

        # get CoM pose in link frame
        com_pos_b = self.data.body_com_pos_b[local_env_ids, 0, :]
        com_quat_b = self.data.body_com_quat_b[local_env_ids, 0, :]
        # transform input CoM pose to link frame
        root_link_pos, root_link_quat = math_utils.combine_frame_transforms(
            root_pose[..., :3],
            root_pose[..., 3:7],
            math_utils.quat_apply(math_utils.quat_inv(com_quat_b), -com_pos_b),
            math_utils.quat_inv(com_quat_b),
        )
        root_link_pose = torch.cat((root_link_pos, root_link_quat), dim=-1)

        # write transformed pose in link frame to sim
        self.write_root_link_pose_to_sim(root_link_pose, env_ids=env_ids)

    def write_root_velocity_to_sim(self, root_velocity: torch.Tensor, env_ids: Sequence[int] | None = None):
        """Set the root center of mass velocity over selected environment indices into the simulation.

        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.
        NOTE: This sets the velocity of the root's center of mass rather than the roots frame.

        Args:
            root_velocity: Root center of mass velocities in simulation world frame. Shape is (len(env_ids), 6).
            env_ids: Environment indices. If None, then all indices are used.
        """
        self.write_root_com_velocity_to_sim(root_velocity=root_velocity, env_ids=env_ids)

    def write_root_com_velocity_to_sim(self, root_velocity: torch.Tensor, env_ids: Sequence[int] | None = None):
        """Set the root center of mass velocity over selected environment indices into the simulation.

        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.
        NOTE: This sets the velocity of the root's center of mass rather than the roots frame.

        Args:
            root_velocity: Root center of mass velocities in simulation world frame. Shape is (len(env_ids), 6).
            env_ids: Environment indices. If None, then all indices are used.
        """
        # resolve all indices
        physx_env_ids = env_ids
        if env_ids is None:
            env_ids = slice(None)
            physx_env_ids = self._ALL_INDICES

        # note: we need to do this here since tensors are not set into simulation until step.
        # set into internal buffers
        self._data.root_com_vel_w[env_ids] = root_velocity.clone()
        # update these buffers only if the user is using them. Otherwise this adds to overhead.
        if self._data._root_com_state_w.data is not None:
            self._data.root_com_state_w[env_ids, 7:] = self._data.root_com_vel_w[env_ids]
        if self._data._root_state_w.data is not None:
            self._data.root_state_w[env_ids, 7:] = self._data.root_com_vel_w[env_ids]
        if self._data._root_link_state_w.data is not None:
            self._data.root_link_state_w[env_ids, 7:] = self._data.root_com_vel_w[env_ids]
        # make the acceleration zero to prevent reporting old values
        self._data.body_com_acc_w[env_ids] = 0.0
        # set into simulation
        self.root_physx_view.set_velocities(self._data.root_com_vel_w, indices=physx_env_ids)

    def write_root_link_velocity_to_sim(self, root_velocity: torch.Tensor, env_ids: Sequence[int] | None = None):
        """Set the root link velocity over selected environment indices into the simulation.

        The velocity comprises linear velocity (x, y, z) and angular velocity (x, y, z) in that order.
        NOTE: This sets the velocity of the root's frame rather than the roots center of mass.

        Args:
            root_velocity: Root frame velocities in simulation world frame. Shape is (len(env_ids), 6).
            env_ids: Environment indices. If None, then all indices are used.
        """
        # resolve all indices
        if env_ids is None:
            local_env_ids = slice(env_ids)
        else:
            local_env_ids = env_ids

        # set into internal buffers
        self._data.root_link_vel_w[local_env_ids] = root_velocity.clone()
        # update these buffers only if the user is using them. Otherwise this adds to overhead.
        if self._data._root_link_state_w.data is not None:
            self._data.root_link_state_w[local_env_ids, 7:] = self._data.root_link_vel_w[local_env_ids]

        # get CoM pose in link frame
        quat = self.data.root_link_quat_w[local_env_ids]
        com_pos_b = self.data.body_com_pos_b[local_env_ids, 0, :]
        # transform input velocity to center of mass frame
        root_com_velocity = root_velocity.clone()
        root_com_velocity[:, :3] += torch.linalg.cross(
            root_com_velocity[:, 3:], math_utils.quat_apply(quat, com_pos_b), dim=-1
        )

        # write transformed velocity in CoM frame to sim
        self.write_root_com_velocity_to_sim(root_com_velocity, env_ids=env_ids)

    """
    Operations - Setters.
    """

    def set_external_force_and_torque(
        self,
        forces: torch.Tensor,
        torques: torch.Tensor,
        body_ids: Sequence[int] | slice | None = None,
        env_ids: Sequence[int] | None = None,
    ):
        """Set external force and torque to apply on the asset's bodies in their local frame.

        For many applications, we want to keep the applied external force on rigid bodies constant over a period of
        time (for instance, during the policy control). This function allows us to store the external force and torque
        into buffers which are then applied to the simulation at every step.

        .. caution::
            If the function is called with empty forces and torques, then this function disables the application
            of external wrench to the simulation.

            .. code-block:: python

                # example of disabling external wrench
                asset.set_external_force_and_torque(forces=torch.zeros(0, 3), torques=torch.zeros(0, 3))

        .. note::
            This function does not apply the external wrench to the simulation. It only fills the buffers with
            the desired values. To apply the external wrench, call the :meth:`write_data_to_sim` function
            right before the simulation step.

        Args:
            forces: External forces in bodies' local frame. Shape is (len(env_ids), len(body_ids), 3).
            torques: External torques in bodies' local frame. Shape is (len(env_ids), len(body_ids), 3).
            body_ids: Body indices to apply external wrench to. Defaults to None (all bodies).
            env_ids: Environment indices to apply external wrench to. Defaults to None (all instances).
        """
        if forces.any() or torques.any():
            self.has_external_wrench = True
        else:
            self.has_external_wrench = False
            # to be safe, explicitly set value to zero
            forces = torques = 0.0

        # resolve all indices
        # -- env_ids
        if env_ids is None:
            env_ids = slice(None)
        # -- body_ids
        if body_ids is None:
            body_ids = slice(None)
        # broadcast env_ids if needed to allow double indexing
        if env_ids != slice(None) and body_ids != slice(None):
            env_ids = env_ids[:, None]
        # set into internal buffers
        self._external_force_b[env_ids, body_ids] = forces
        self._external_torque_b[env_ids, body_ids] = torques

    """
    Internal helper.
    """

    def _initialize_impl(self):
        # obtain global simulation view
        self._physics_sim_view = SimulationManager.get_physics_sim_view()
        # obtain the first prim in the regex expression (all others are assumed to be a copy of this)
        template_prim = sim_utils.find_first_matching_prim(self.cfg.prim_path)
        if template_prim is None:
            raise RuntimeError(f"Failed to find prim for expression: '{self.cfg.prim_path}'.")
        template_prim_path = template_prim.GetPath().pathString

        # find rigid root prims
        root_prims = sim_utils.get_all_matching_child_prims(
            template_prim_path, predicate=lambda prim: prim.HasAPI(UsdPhysics.RigidBodyAPI)
        )
        if len(root_prims) == 0:
            raise RuntimeError(
                f"Failed to find a rigid body when resolving '{self.cfg.prim_path}'."
                " Please ensure that the prim has 'USD RigidBodyAPI' applied."
            )
        if len(root_prims) > 1:
            raise RuntimeError(
                f"Failed to find a single rigid body when resolving '{self.cfg.prim_path}'."
                f" Found multiple '{root_prims}' under '{template_prim_path}'."
                " Please ensure that there is only one rigid body in the prim path tree."
            )

        articulation_prims = sim_utils.get_all_matching_child_prims(
            template_prim_path, predicate=lambda prim: prim.HasAPI(UsdPhysics.ArticulationRootAPI)
        )
        if len(articulation_prims) != 0:
            if articulation_prims[0].GetAttribute("physxArticulation:articulationEnabled").Get():
                raise RuntimeError(
                    f"Found an articulation root when resolving '{self.cfg.prim_path}' for rigid objects. These are"
                    f" located at: '{articulation_prims}' under '{template_prim_path}'. Please disable the articulation"
                    " root in the USD or from code by setting the parameter"
                    " 'ArticulationRootPropertiesCfg.articulation_enabled' to False in the spawn configuration."
                )

        # resolve root prim back into regex expression
        root_prim_path = root_prims[0].GetPath().pathString
        root_prim_path_expr = self.cfg.prim_path + root_prim_path[len(template_prim_path) :]
        # -- object view
        self._root_physx_view = self._physics_sim_view.create_rigid_body_view(root_prim_path_expr.replace(".*", "*"))

        # check if the rigid body was created
        if self._root_physx_view._backend is None:
            raise RuntimeError(f"Failed to create rigid body at: {self.cfg.prim_path}. Please check PhysX logs.")

        # log information about the rigid body
        omni.log.info(f"Rigid body initialized at: {self.cfg.prim_path} with root '{root_prim_path_expr}'.")
        omni.log.info(f"Number of instances: {self.num_instances}")
        omni.log.info(f"Number of bodies: {self.num_bodies}")
        omni.log.info(f"Body names: {self.body_names}")

        # container for data access
        self._data = RigidObjectData(self.root_physx_view, self.device)

        # create buffers
        self._create_buffers()
        # process configuration
        self._process_cfg()
        # update the rigid body data
        self.update(0.0)

    def _create_buffers(self):
        """Create buffers for storing data."""
        # constants
        self._ALL_INDICES = torch.arange(self.num_instances, dtype=torch.long, device=self.device)

        # external forces and torques
        self.has_external_wrench = False
        self._external_force_b = torch.zeros((self.num_instances, self.num_bodies, 3), device=self.device)
        self._external_torque_b = torch.zeros_like(self._external_force_b)

        # set information about rigid body into data
        self._data.body_names = self.body_names
        self._data.default_mass = self.root_physx_view.get_masses().clone()
        self._data.default_inertia = self.root_physx_view.get_inertias().clone()

    def _process_cfg(self):
        """Post processing of configuration parameters."""
        # default state
        # -- root state
        # note: we cast to tuple to avoid torch/numpy type mismatch.
        default_root_state = (
            tuple(self.cfg.init_state.pos)
            + tuple(self.cfg.init_state.rot)
            + tuple(self.cfg.init_state.lin_vel)
            + tuple(self.cfg.init_state.ang_vel)
        )
        default_root_state = torch.tensor(default_root_state, dtype=torch.float, device=self.device)
        self._data.default_root_state = default_root_state.repeat(self.num_instances, 1)

    """
    Internal simulation callbacks.
    """

    def _invalidate_initialize_callback(self, event):
        """Invalidates the scene elements."""
        # call parent
        super()._invalidate_initialize_callback(event)
        # set all existing views to None to invalidate them
        self._root_physx_view = None

SCENE ENTITY CFG:
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""Configuration terms for different managers."""

from dataclasses import MISSING

from isaaclab.assets import Articulation, RigidObject, RigidObjectCollection
from isaaclab.scene import InteractiveScene
from isaaclab.utils import configclass


@configclass
class SceneEntityCfg:
    """Configuration for a scene entity that is used by the manager's term.

    This class is used to specify the name of the scene entity that is queried from the
    :class:`InteractiveScene` and passed to the manager's term function.
    """

    name: str = MISSING
    """The name of the scene entity.

    This is the name defined in the scene configuration file. See the :class:`InteractiveSceneCfg`
    class for more details.
    """

    joint_names: str | list[str] | None = None
    """The names of the joints from the scene entity. Defaults to None.

    The names can be either joint names or a regular expression matching the joint names.

    These are converted to joint indices on initialization of the manager and passed to the term
    function as a list of joint indices under :attr:`joint_ids`.
    """

    joint_ids: list[int] | slice = slice(None)
    """The indices of the joints from the asset required by the term. Defaults to slice(None), which means
    all the joints in the asset (if present).

    If :attr:`joint_names` is specified, this is filled in automatically on initialization of the
    manager.
    """

    fixed_tendon_names: str | list[str] | None = None
    """The names of the fixed tendons from the scene entity. Defaults to None.

    The names can be either joint names or a regular expression matching the joint names.

    These are converted to fixed tendon indices on initialization of the manager and passed to the term
    function as a list of fixed tendon indices under :attr:`fixed_tendon_ids`.
    """

    fixed_tendon_ids: list[int] | slice = slice(None)
    """The indices of the fixed tendons from the asset required by the term. Defaults to slice(None), which means
    all the fixed tendons in the asset (if present).

    If :attr:`fixed_tendon_names` is specified, this is filled in automatically on initialization of the
    manager.
    """

    body_names: str | list[str] | None = None
    """The names of the bodies from the asset required by the term. Defaults to None.

    The names can be either body names or a regular expression matching the body names.

    These are converted to body indices on initialization of the manager and passed to the term
    function as a list of body indices under :attr:`body_ids`.
    """

    body_ids: list[int] | slice = slice(None)
    """The indices of the bodies from the asset required by the term. Defaults to slice(None), which means
    all the bodies in the asset.

    If :attr:`body_names` is specified, this is filled in automatically on initialization of the
    manager.
    """

    object_collection_names: str | list[str] | None = None
    """The names of the objects in the rigid object collection required by the term. Defaults to None.

    The names can be either names or a regular expression matching the object names in the collection.

    These are converted to object indices on initialization of the manager and passed to the term
    function as a list of object indices under :attr:`object_collection_ids`.
    """

    object_collection_ids: list[int] | slice = slice(None)
    """The indices of the objects from the rigid object collection required by the term. Defaults to slice(None),
    which means all the objects in the collection.

    If :attr:`object_collection_names` is specified, this is filled in automatically on initialization of the manager.
    """

    preserve_order: bool = False
    """Whether to preserve indices ordering to match with that in the specified joint, body, or object collection names.
    Defaults to False.

    If False, the ordering of the indices are sorted in ascending order (i.e. the ordering in the entity's joints,
    bodies, or object in the object collection). Otherwise, the indices are preserved in the order of the specified
    joint, body, or object collection names.

    For more details, see the :meth:`isaaclab.utils.string.resolve_matching_names` function.

    .. note::
        This attribute is only used when :attr:`joint_names`, :attr:`body_names`, or :attr:`object_collection_names` are specified.

    """

    def resolve(self, scene: InteractiveScene):
        """Resolves the scene entity and converts the joint and body names to indices.

        This function examines the scene entity from the :class:`InteractiveScene` and resolves the indices
        and names of the joints and bodies. It is an expensive operation as it resolves regular expressions
        and should be called only once.

        Args:
            scene: The interactive scene instance.

        Raises:
            ValueError: If the scene entity is not found.
            ValueError: If both ``joint_names`` and ``joint_ids`` are specified and are not consistent.
            ValueError: If both ``fixed_tendon_names`` and ``fixed_tendon_ids`` are specified and are not consistent.
            ValueError: If both ``body_names`` and ``body_ids`` are specified and are not consistent.
            ValueError: If both ``object_collection_names`` and ``object_collection_ids`` are specified and are not consistent.
        """
        # check if the entity is valid
        if self.name not in scene.keys():
            raise ValueError(f"The scene entity '{self.name}' does not exist. Available entities: {scene.keys()}.")

        # convert joint names to indices based on regex
        self._resolve_joint_names(scene)

        # convert fixed tendon names to indices based on regex
        self._resolve_fixed_tendon_names(scene)

        # convert body names to indices based on regex
        self._resolve_body_names(scene)

        # convert object collection names to indices based on regex
        self._resolve_object_collection_names(scene)

    def _resolve_joint_names(self, scene: InteractiveScene):
        # convert joint names to indices based on regex
        if self.joint_names is not None or self.joint_ids != slice(None):
            entity: Articulation = scene[self.name]
            # -- if both are not their default values, check if they are valid
            if self.joint_names is not None and self.joint_ids != slice(None):
                if isinstance(self.joint_names, str):
                    self.joint_names = [self.joint_names]
                if isinstance(self.joint_ids, int):
                    self.joint_ids = [self.joint_ids]
                joint_ids, _ = entity.find_joints(self.joint_names, preserve_order=self.preserve_order)
                joint_names = [entity.joint_names[i] for i in self.joint_ids]
                if joint_ids != self.joint_ids or joint_names != self.joint_names:
                    raise ValueError(
                        "Both 'joint_names' and 'joint_ids' are specified, and are not consistent."
                        f"\n\tfrom joint names: {self.joint_names} [{joint_ids}]"
                        f"\n\tfrom joint ids: {joint_names} [{self.joint_ids}]"
                        "\nHint: Use either 'joint_names' or 'joint_ids' to avoid confusion."
                    )
            # -- from joint names to joint indices
            elif self.joint_names is not None:
                if isinstance(self.joint_names, str):
                    self.joint_names = [self.joint_names]
                self.joint_ids, _ = entity.find_joints(self.joint_names, preserve_order=self.preserve_order)
                # performance optimization (slice offers faster indexing than list of indices)
                # only all joint in the entity order are selected
                if len(self.joint_ids) == entity.num_joints and self.joint_names == entity.joint_names:
                    self.joint_ids = slice(None)
            # -- from joint indices to joint names
            elif self.joint_ids != slice(None):
                if isinstance(self.joint_ids, int):
                    self.joint_ids = [self.joint_ids]
                self.joint_names = [entity.joint_names[i] for i in self.joint_ids]

    def _resolve_fixed_tendon_names(self, scene: InteractiveScene):
        # convert tendon names to indices based on regex
        if self.fixed_tendon_names is not None or self.fixed_tendon_ids != slice(None):
            entity: Articulation = scene[self.name]
            # -- if both are not their default values, check if they are valid
            if self.fixed_tendon_names is not None and self.fixed_tendon_ids != slice(None):
                if isinstance(self.fixed_tendon_names, str):
                    self.fixed_tendon_names = [self.fixed_tendon_names]
                if isinstance(self.fixed_tendon_ids, int):
                    self.fixed_tendon_ids = [self.fixed_tendon_ids]
                fixed_tendon_ids, _ = entity.find_fixed_tendons(
                    self.fixed_tendon_names, preserve_order=self.preserve_order
                )
                fixed_tendon_names = [entity.fixed_tendon_names[i] for i in self.fixed_tendon_ids]
                if fixed_tendon_ids != self.fixed_tendon_ids or fixed_tendon_names != self.fixed_tendon_names:
                    raise ValueError(
                        "Both 'fixed_tendon_names' and 'fixed_tendon_ids' are specified, and are not consistent."
                        f"\n\tfrom joint names: {self.fixed_tendon_names} [{fixed_tendon_ids}]"
                        f"\n\tfrom joint ids: {fixed_tendon_names} [{self.fixed_tendon_ids}]"
                        "\nHint: Use either 'fixed_tendon_names' or 'fixed_tendon_ids' to avoid confusion."
                    )
            # -- from fixed tendon names to fixed tendon indices
            elif self.fixed_tendon_names is not None:
                if isinstance(self.fixed_tendon_names, str):
                    self.fixed_tendon_names = [self.fixed_tendon_names]
                self.fixed_tendon_ids, _ = entity.find_fixed_tendons(
                    self.fixed_tendon_names, preserve_order=self.preserve_order
                )
                # performance optimization (slice offers faster indexing than list of indices)
                # only all fixed tendon in the entity order are selected
                if (
                    len(self.fixed_tendon_ids) == entity.num_fixed_tendons
                    and self.fixed_tendon_names == entity.fixed_tendon_names
                ):
                    self.fixed_tendon_ids = slice(None)
            # -- from fixed tendon indices to fixed tendon names
            elif self.fixed_tendon_ids != slice(None):
                if isinstance(self.fixed_tendon_ids, int):
                    self.fixed_tendon_ids = [self.fixed_tendon_ids]
                self.fixed_tendon_names = [entity.fixed_tendon_names[i] for i in self.fixed_tendon_ids]

    def _resolve_body_names(self, scene: InteractiveScene):
        # convert body names to indices based on regex
        if self.body_names is not None or self.body_ids != slice(None):
            entity: RigidObject = scene[self.name]
            # -- if both are not their default values, check if they are valid
            if self.body_names is not None and self.body_ids != slice(None):
                if isinstance(self.body_names, str):
                    self.body_names = [self.body_names]
                if isinstance(self.body_ids, int):
                    self.body_ids = [self.body_ids]
                body_ids, _ = entity.find_bodies(self.body_names, preserve_order=self.preserve_order)
                body_names = [entity.body_names[i] for i in self.body_ids]
                if body_ids != self.body_ids or body_names != self.body_names:
                    raise ValueError(
                        "Both 'body_names' and 'body_ids' are specified, and are not consistent."
                        f"\n\tfrom body names: {self.body_names} [{body_ids}]"
                        f"\n\tfrom body ids: {body_names} [{self.body_ids}]"
                        "\nHint: Use either 'body_names' or 'body_ids' to avoid confusion."
                    )
            # -- from body names to body indices
            elif self.body_names is not None:
                if isinstance(self.body_names, str):
                    self.body_names = [self.body_names]
                self.body_ids, _ = entity.find_bodies(self.body_names, preserve_order=self.preserve_order)
                # performance optimization (slice offers faster indexing than list of indices)
                # only all bodies in the entity order are selected
                if len(self.body_ids) == entity.num_bodies and self.body_names == entity.body_names:
                    self.body_ids = slice(None)
            # -- from body indices to body names
            elif self.body_ids != slice(None):
                if isinstance(self.body_ids, int):
                    self.body_ids = [self.body_ids]
                self.body_names = [entity.body_names[i] for i in self.body_ids]

    def _resolve_object_collection_names(self, scene: InteractiveScene):
        # convert object names to indices based on regex
        if self.object_collection_names is not None or self.object_collection_ids != slice(None):
            entity: RigidObjectCollection = scene[self.name]
            # -- if both are not their default values, check if they are valid
            if self.object_collection_names is not None and self.object_collection_ids != slice(None):
                if isinstance(self.object_collection_names, str):
                    self.object_collection_names = [self.object_collection_names]
                if isinstance(self.object_collection_ids, int):
                    self.object_collection_ids = [self.object_collection_ids]
                object_ids, _ = entity.find_objects(self.object_collection_names, preserve_order=self.preserve_order)
                object_names = [entity.object_names[i] for i in self.object_collection_ids]
                if object_ids != self.object_collection_ids or object_names != self.object_collection_names:
                    raise ValueError(
                        "Both 'object_collection_names' and 'object_collection_ids' are specified, and are not"
                        " consistent.\n\tfrom object collection names:"
                        f" {self.object_collection_names} [{object_ids}]\n\tfrom object collection ids:"
                        f" {object_names} [{self.object_collection_ids}]\nHint: Use either 'object_collection_names' or"
                        " 'object_collection_ids' to avoid confusion."
                    )
            # -- from object names to object indices
            elif self.object_collection_names is not None:
                if isinstance(self.object_collection_names, str):
                    self.object_collection_names = [self.object_collection_names]
                self.object_collection_ids, _ = entity.find_objects(
                    self.object_collection_names, preserve_order=self.preserve_order
                )
            # -- from object indices to object names
            elif self.object_collection_ids != slice(None):
                if isinstance(self.object_collection_ids, int):
                    self.object_collection_ids = [self.object_collection_ids]
                self.object_collection_names = [entity.object_names[i] for i in self.object_collection_ids]

'''
</content>
</template_file>

<template_file>
<path>task_name/__init__.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""Configurations for the object stack environments."""

# We leave this file empty since we don't want to expose any configs in this package directly.
# We still need this file to import the "config" module in the parent package.

</content>
</template_file>

<template_file>
<path>task_name/config/__init__.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""Configurations for the cabinet environments."""

# We leave this file empty since we don't want to expose any configs in this package directly.
# We still need this file to import the "config" module in the parent package.
</content>
</template_file>

<template_file>
<path>task_name/config/franka/joint_pos_env_cfg.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

from isaaclab.sensors import FrameTransformerCfg
from isaaclab.sensors.frame_transformer.frame_transformer_cfg import OffsetCfg
from isaaclab.utils import configclass
# TODO: Import task-specific MDP module
# from isaaclab_tasks.manager_based.{TASK_CATEGORY}.{TASK_NAME} import mdp
from isaaclab_tasks.manager_based.{TASK_CATEGORY}.{TASK_NAME}.{TASK_NAME}_env_cfg import (  # isort: skip
    FRAME_MARKER_SMALL_CFG,
    {TASK_NAME_PASCAL}EnvCfg,
)

##
# Pre-defined configs
##
from isaaclab_assets.robots.franka import FRANKA_PANDA_CFG  # isort: skip


@configclass
class Franka{TASK_NAME_PASCAL}EnvCfg({TASK_NAME_PASCAL}EnvCfg):
    """Configuration for Franka Panda robot performing {TASK_NAME} task."""
    
    def __post_init__(self):
        # post init of parent
        super().__post_init__()
        
        # Set franka as robot
        self.scene.robot = FRANKA_PANDA_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
        
        # Set Actions for the specific robot type (franka)
        self.actions.arm_action = mdp.JointPositionActionCfg(
            asset_name="robot",
            joint_names=["panda_joint.*"],
            scale=1.0,
            use_default_offset=True,
        )
        self.actions.gripper_action = mdp.BinaryJointPositionActionCfg(
            asset_name="robot",
            joint_names=["panda_finger.*"],
            open_command_expr={"panda_finger_.*": 0.04},
            close_command_expr={"panda_finger_.*": 0.0},
        )

        # Set the body name for the end effector
        self.commands.object_pose.body_name = "panda_hand"

        # TODO: Implement the objects in the scene and their positions that we defined as MISSING in the environment config file
        #Here is an example of such an object:
        '''
        self.scene.object = RigidObjectCfg(
            prim_path="{ENV_REGEX_NS}/Object",
            init_state=RigidObjectCfg.InitialStateCfg(pos=[0.5, 0, 0.055], rot=[1, 0, 0, 0]),
            spawn=UsdFileCfg(
                usd_path=f"{ISAAC_NUCLEUS_DIR}/Props/Blocks/DexCube/dex_cube_instanceable.usd",
                scale=(0.8, 0.8, 0.8),
                rigid_props=RigidBodyPropertiesCfg(
                    solver_position_iteration_count=16,
                    solver_velocity_iteration_count=1,
                    max_angular_velocity=1000.0,
                    max_linear_velocity=1000.0,
                    max_depenetration_velocity=5.0,
                    disable_gravity=False,
                ),
            ),
        )
        '''

        # Listens to the required transforms
        # IMPORTANT: The order of the frames in the list is important. The first frame is the tool center point (TCP)
        # the other frames are the fingers
        # No need tgo change this
        self.scene.ee_frame = FrameTransformerCfg(
            prim_path="{ENV_REGEX_NS}/Robot/panda_link0",
            debug_vis=False,
            visualizer_cfg=FRAME_MARKER_SMALL_CFG.replace(prim_path="/Visuals/EndEffectorFrameTransformer"),
            target_frames=[
                FrameTransformerCfg.FrameCfg(
                    prim_path="{ENV_REGEX_NS}/Robot/panda_hand",
                    name="ee_tcp",
                    offset=OffsetCfg(
                        pos=(0.0, 0.0, 0.1034),
                    ),
                ),
                FrameTransformerCfg.FrameCfg(
                    prim_path="{ENV_REGEX_NS}/Robot/panda_leftfinger",
                    name="tool_leftfinger",
                    offset=OffsetCfg(
                        pos=(0.0, 0.0, 0.046),
                    ),
                ),
                FrameTransformerCfg.FrameCfg(
                    prim_path="{ENV_REGEX_NS}/Robot/panda_rightfinger",
                    name="tool_rightfinger",
                    offset=OffsetCfg(
                        pos=(0.0, 0.0, 0.046),
                    ),
                ),
            ],
        )
        
        # TODO: Override task-specific reward parameters IF and only IF required
        # Example reward overrides for {TASK_NAME}:
        # self.rewards.{REWARD_NAME}.params["offset"] = {OFFSET_VALUE}
        # self.rewards.{GRASP_REWARD}.params["open_joint_pos"] = 0.04
        # self.rewards.{GRASP_REWARD}.params["asset_cfg"].joint_names = ["panda_finger_.*"]


@configclass
class Franka{TASK_NAME_PASCAL}EnvCfg_PLAY(Franka{TASK_NAME_PASCAL}EnvCfg):
    """Play/demo configuration for Franka Panda robot performing {TASK_NAME} task with reduced complexity."""
    
    def __post_init__(self):
        # post init of parent
        super().__post_init__()
        
        # TODO: Configure scene for play/demo mode
        self.scene.num_envs = {PLAY_NUM_ENVS}  # e.g., 50
        self.scene.env_spacing = {PLAY_ENV_SPACING}  # e.g., 2.5
        
        # disable randomization for play
        self.observations.policy.enable_corruption = False


# TODO: Configuration Instructions for LLM:
# 
# When populating this template, replace the following placeholders:
# 
# TASK-SPECIFIC:
# - {TASK_CATEGORY}: Task category (e.g., manipulation, locomotion, navigation)
# - {TASK_NAME}: Task name in lowercase (e.g., cabinet, pick_and_place, door_opening)
# - {TASK_NAME_PASCAL}: Task name in PascalCase (e.g., Cabinet, PickAndPlace, DoorOpening)
# 
# REWARD PARAMETERS (uncomment and populate specific reward overrides):
# - {REWARD_NAME}: Name of reward component to override (e.g., approach_gripper_handle)
# - {GRASP_REWARD}: Name of grasp-related reward component (e.g., grasp_handle)
# - {OFFSET_VALUE}: Offset value for reward calculation (float, e.g., 0.04)
# 
# PLAY CONFIGURATION:
# - {PLAY_NUM_ENVS}: Number of environments for play mode (int, e.g., 50)
# - {PLAY_ENV_SPACING}: Spacing between environments in meters (float, e.g., 2.5)
# 
# Notes:
# 1. Remove TODO comments after populating
# 2. Add task-specific reward overrides in the rewards section
# 3. The Franka Panda robot configuration (joints, links, offsets) remains constant
# 4. Only task-specific parameters need to be customized
# 5. Import the correct task MDP module
# 6. Ensure reward parameter names match the task's reward configuration
</content>
</template_file>

<template_file>
<path>task_name/config/franka/__init__.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""Configuration for task name environments with Franka robot."""

import gymnasium as gym

from . import agents

gym.register(
    # TODO: rename in a way that makes sense for the task for id
    id="Isaac-Open-Drawer-Franka-v0",
    entry_point="isaaclab.envs:ManagerBasedRLEnv",
    kwargs={
        # TODO: rename the cabinet to whatever name makes sense for the task
        "env_cfg_entry_point": f"{__name__}.joint_pos_env_cfg:FrankaCabinetEnvCfg",
        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:CabinetPPORunnerCfg",
        "rl_games_cfg_entry_point": f"{agents.__name__}:rl_games_ppo_cfg.yaml",
        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_ppo_cfg.yaml",
    },
    disable_env_checker=True,
)

# TODO: renaming to match with above name
gym.register(
    id="Isaac-Open-Drawer-Franka-Play-v0",
    entry_point="isaaclab.envs:ManagerBasedRLEnv",
    kwargs={
        "env_cfg_entry_point": f"{__name__}.joint_pos_env_cfg:FrankaCabinetEnvCfg_PLAY",
        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:CabinetPPORunnerCfg",
        "rl_games_cfg_entry_point": f"{agents.__name__}:rl_games_ppo_cfg.yaml",
        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_ppo_cfg.yaml",
    },
    disable_env_checker=True,
)
</content>
</template_file>

<template_file>
<path>task_name/config/franka/agents/rsl_rl_ppo_cfg.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

from isaaclab.utils import configclass
from isaaclab_rl.rsl_rl import RslRlOnPolicyRunnerCfg, RslRlPpoActorCriticCfg, RslRlPpoAlgorithmCfg


@configclass
class TaskNameRslRlPPORunnerCfg(RslRlPPORunnerCfg):
    """Configuration for the RSL-RL PPO runner."""
    # TODO: Define training parameters
    # Example:
    # max_iterations = 1000  # TODO
    # save_interval = 50  # TODO
    # experiment_name = "task_name_ppo"  # TODO
    # run_name = "task_name_ppo_run"  # TODO
    # load_run = -1  # TODO
    # checkpoint = "model_*.pt"  # TODO
    # num_steps_per_env = 24  # TODO
    # max_episode_length = 1000  # TODO
    # seed = 1  # TODO
    # num_envs = 4096  # TODO
    # resume = False  # TODO
    # ... (add more as needed) 
</content>
</template_file>

<template_file>
<path>task_name/config/franka/agents/__init__.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

</content>
</template_file>

<template_file>
<path>task_name/mdp/observations.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

import isaaclab.utils.math as math_utils
from isaaclab.assets import ArticulationData, RigidObjectData
from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import FrameTransformerData
from isaaclab.utils.math import subtract_frame_transforms

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


# TODO: Add task-specific observation functions below
# Follow the existing patterns for consistency

def object_position_in_robot_root_frame(
    env: ManagerBasedRLEnv,
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    object_cfg: SceneEntityCfg = SceneEntityCfg("{MAIN_OBJECT_NAME}"),  # TODO: Replace with main interaction object
) -> torch.Tensor:
    """The position of the {MAIN_OBJECT_NAME} in the robot's root frame.
    
    This function computes the position of the main interaction object relative to the robot's
    base frame, which is useful for understanding spatial relationships independent of the
    robot's position in the world.
    
    Args:
        env: The environment instance.
        robot_cfg: Configuration for the robot entity. Defaults to SceneEntityCfg("robot").
        object_cfg: Configuration for the main object entity. Defaults to SceneEntityCfg("{MAIN_OBJECT_NAME}").
        
    Returns:
        The position of the {MAIN_OBJECT_NAME} in the robot's root frame as a tensor of shape (num_envs, 3).
    """
    robot = env.scene[robot_cfg.name]
    main_object = env.scene[object_cfg.name]
    object_pos_w = main_object.data.root_pos_w[:, :3]
    object_pos_b, *_ = subtract_frame_transforms(robot.data.root_pos_w, robot.data.root_quat_w, object_pos_w)
    return object_pos_b


def rel_ee_object_distance(env: ManagerBasedRLEnv) -> torch.Tensor:
    """The distance between the end-effector and the main interaction object.
    
    This observation provides the relative position vector from the end-effector to the
    main object, which is crucial for approach and manipulation behaviors.
    
    Args:
        env: The environment instance.
        
    Returns:
        The distance vector from end-effector to object as a tensor of shape (num_envs, 3).
    """
    ee_tf_data: FrameTransformerData = env.scene["ee_frame"].data
    object_data: ArticulationData = env.scene["{MAIN_OBJECT_NAME}"].data  # TODO: Replace with main object name
    return object_data.root_pos_w - ee_tf_data.target_pos_w[..., 0, :]


# TODO: Add task-specific distance functions
# Example template for additional objects:
def rel_ee_{SECONDARY_OBJECT_NAME}_distance(env: ManagerBasedRLEnv) -> torch.Tensor:
    """The distance between the end-effector and the {SECONDARY_OBJECT_NAME}.
    
    Args:
        env: The environment instance.
        
    Returns:
        The distance vector from end-effector to {SECONDARY_OBJECT_NAME} as a tensor of shape (num_envs, 3).
    """
    ee_tf_data: FrameTransformerData = env.scene["ee_frame"].data
    secondary_object_data: FrameTransformerData = env.scene["{SECONDARY_OBJECT_FRAME_NAME}"].data  # TODO: Replace with secondary object frame name
    return secondary_object_data.target_pos_w[..., 0, :] - ee_tf_data.target_pos_w[..., 0, :]


def fingertips_pos(env: ManagerBasedRLEnv) -> torch.Tensor:
    """The position of the fingertips relative to the environment origins.
    
    This function provides the absolute positions of both fingertips in the environment,
    which is useful for fine manipulation tasks and grasp analysis.
    
    Args:
        env: The environment instance.
        
    Returns:
        The flattened positions of all fingertips as a tensor of shape (num_envs, 6).
        The tensor contains [left_finger_x, left_finger_y, left_finger_z, right_finger_x, right_finger_y, right_finger_z].
    """
    ee_tf_data: FrameTransformerData = env.scene["ee_frame"].data
    fingertips_pos = ee_tf_data.target_pos_w[..., 1:, :] - env.scene.env_origins.unsqueeze(1)
    return fingertips_pos.view(env.num_envs, -1)


def ee_pos(env: ManagerBasedRLEnv) -> torch.Tensor:
    """The position of the end-effector relative to the environment origins.
    
    This provides the absolute position of the robot's end-effector in the environment,
    which is fundamental for most manipulation tasks.
    
    Args:
        env: The environment instance.
        
    Returns:
        The end-effector position as a tensor of shape (num_envs, 3).
    """
    ee_tf_data: FrameTransformerData = env.scene["ee_frame"].data
    ee_pos = ee_tf_data.target_pos_w[..., 0, :] - env.scene.env_origins
    return ee_pos


def ee_quat(env: ManagerBasedRLEnv, make_quat_unique: bool = True) -> torch.Tensor:
    """The orientation of the end-effector in the environment frame.
    
    This provides the orientation of the robot's end-effector, which is important for
    tasks that require specific approach angles or orientations.
    
    Args:
        env: The environment instance.
        make_quat_unique: If True, the quaternion is made unique by ensuring the real part is positive.
        
    Returns:
        The end-effector quaternion as a tensor of shape (num_envs, 4).
    """
    ee_tf_data: FrameTransformerData = env.scene["ee_frame"].data
    ee_quat = ee_tf_data.target_quat_w[..., 0, :]
    # make first element of quaternion positive
    return math_utils.quat_unique(ee_quat) if make_quat_unique else ee_quat


# TODO: Add task-specific state observation functions
# Examples of common patterns:

# def {OBJECT_NAME}_position(env: ManagerBasedRLEnv) -> torch.Tensor:
#     """The position of the {OBJECT_NAME} relative to the environment origins.
#     
#     Args:
#         env: The environment instance.
#         
#     Returns:
#         The {OBJECT_NAME} position as a tensor of shape (num_envs, 3).
#     """
#     object_data = env.scene["{OBJECT_NAME}"].data
#     return object_data.root_pos_w - env.scene.env_origins

# def {OBJECT_NAME}_orientation(env: ManagerBasedRLEnv, make_quat_unique: bool = True) -> torch.Tensor:
#     """The orientation of the {OBJECT_NAME} in the environment frame.
#     
#     Args:
#         env: The environment instance.
#         make_quat_unique: If True, the quaternion is made unique by ensuring the real part is positive.
#         
#     Returns:
#         The {OBJECT_NAME} quaternion as a tensor of shape (num_envs, 4).
#     """
#     object_data = env.scene["{OBJECT_NAME}"].data
#     object_quat = object_data.root_quat_w
#     return math_utils.quat_unique(object_quat) if make_quat_unique else object_quat

# def {JOINT_NAME}_joint_state(env: ManagerBasedRLEnv) -> torch.Tensor:
#     """The joint state (position and velocity) of {JOINT_NAME}.
#     
#     Args:
#         env: The environment instance.
#         
#     Returns:
#         The joint state as a tensor of shape (num_envs, 2) containing [position, velocity].
#     """
#     articulation_data: ArticulationData = env.scene["{ARTICULATION_NAME}"].data
#     joint_pos = articulation_data.joint_pos[:, {JOINT_INDEX}]  # TODO: Replace with correct joint index
#     joint_vel = articulation_data.joint_vel[:, {JOINT_INDEX}]  # TODO: Replace with correct joint index
#     return torch.stack([joint_pos, joint_vel], dim=-1)


# TODO: Configuration Instructions for LLM:
# 
# When populating this template, replace the following placeholders:
# 
# OBJECT-SPECIFIC:
# - {MAIN_OBJECT_NAME}: Primary interaction object name (e.g., "object", "cabinet", "door")
# - {SECONDARY_OBJECT_NAME}: Secondary object name in function names (e.g., "drawer", "handle", "button")
# - {SECONDARY_OBJECT_FRAME_NAME}: Frame name for secondary object (e.g., "cabinet_frame", "door_frame")
# 
# TASK-SPECIFIC PATTERNS:
# - {OBJECT_NAME}: Replace in template functions with specific object names
# - {JOINT_NAME}: Replace with specific joint names for articulated objects
# - {ARTICULATION_NAME}: Name of articulated asset in scene
# - {JOINT_INDEX}: Integer index of the joint in the articulation
# 
# Notes:
# 1. Keep the existing functions (ee_pos, ee_quat, fingertips_pos) as they're universal
# 2. Customize object_position_in_robot_root_frame and rel_ee_object_distance for your main object
# 3. Add additional rel_ee_*_distance functions for secondary interaction objects
# 4. Uncomment and customize template functions as needed for your specific task
# 5. Remove TODO comments after populating
# 6. Ensure object/frame names match your environment configuration
# 7. Add proper docstrings explaining what each observation represents
# 8. Consider the observation space requirements of your RL algorithm
</content>
</template_file>

<template_file>
<path>task_name/mdp/rewards.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

from isaaclab.assets import RigidObject
from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import FrameTransformer
from isaaclab.utils.math import combine_frame_transforms, matrix_from_quat

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


# TODO: Add task-specific reward functions below
# Use the provided examples as templates and customize for your specific task

# =============================================================================
# EXAMPLE 1: Cabinet Task Functions (Reference Implementation)
# These show patterns for approach, alignment, grasping, and manipulation rewards
# =============================================================================

def approach_ee_handle(env: ManagerBasedRLEnv, threshold: float) -> torch.Tensor:
    r"""Reward the robot for reaching the drawer handle using inverse-square law.

    It uses a piecewise function to reward the robot for reaching the handle.

    .. math::

        reward = \begin{cases}
            2 * (1 / (1 + distance^2))^2 & \text{if } distance \leq threshold \\
            (1 / (1 + distance^2))^2 & \text{otherwise}
        \end{cases}

    """
    ee_tcp_pos = env.scene["ee_frame"].data.target_pos_w[..., 0, :]
    handle_pos = env.scene["cabinet_frame"].data.target_pos_w[..., 0, :]

    # Compute the distance of the end-effector to the handle
    distance = torch.norm(handle_pos - ee_tcp_pos, dim=-1, p=2)

    # Reward the robot for reaching the handle
    reward = 1.0 / (1.0 + distance**2)
    reward = torch.pow(reward, 2)
    return torch.where(distance <= threshold, 2 * reward, reward)


def align_ee_handle(env: ManagerBasedRLEnv) -> torch.Tensor:
    """Reward for aligning the end-effector with the handle.

    The reward is based on the alignment of the gripper with the handle. It is computed as follows:

    .. math::

        reward = 0.5 * (align_z^2 + align_x^2)

    where :math:`align_z` is the dot product of the z direction of the gripper and the -x direction of the handle
    and :math:`align_x` is the dot product of the x direction of the gripper and the -y direction of the handle.
    """
    ee_tcp_quat = env.scene["ee_frame"].data.target_quat_w[..., 0, :]
    handle_quat = env.scene["cabinet_frame"].data.target_quat_w[..., 0, :]

    ee_tcp_rot_mat = matrix_from_quat(ee_tcp_quat)
    handle_mat = matrix_from_quat(handle_quat)

    # get current x and y direction of the handle
    handle_x, handle_y = handle_mat[..., 0], handle_mat[..., 1]
    # get current x and z direction of the gripper
    ee_tcp_x, ee_tcp_z = ee_tcp_rot_mat[..., 0], ee_tcp_rot_mat[..., 2]

    # make sure gripper aligns with the handle
    # in this case, the z direction of the gripper should be close to the -x direction of the handle
    # and the x direction of the gripper should be close to the -y direction of the handle
    # dot product of z and x should be large
    align_z = torch.bmm(ee_tcp_z.unsqueeze(1), -handle_x.unsqueeze(-1)).squeeze(-1).squeeze(-1)
    align_x = torch.bmm(ee_tcp_x.unsqueeze(1), -handle_y.unsqueeze(-1)).squeeze(-1).squeeze(-1)
    return 0.5 * (torch.sign(align_z) * align_z**2 + torch.sign(align_x) * align_x**2)


def align_grasp_around_handle(env: ManagerBasedRLEnv) -> torch.Tensor:
    """Bonus for correct hand orientation around the handle.

    The correct hand orientation is when the left finger is above the handle and the right finger is below the handle.
    """
    # Target object position: (num_envs, 3)
    handle_pos = env.scene["cabinet_frame"].data.target_pos_w[..., 0, :]
    # Fingertips position: (num_envs, n_fingertips, 3)
    ee_fingertips_w = env.scene["ee_frame"].data.target_pos_w[..., 1:, :]
    lfinger_pos = ee_fingertips_w[..., 0, :]
    rfinger_pos = ee_fingertips_w[..., 1, :]

    # Check if hand is in a graspable pose
    is_graspable = (rfinger_pos[:, 2] < handle_pos[:, 2]) & (lfinger_pos[:, 2] > handle_pos[:, 2])

    # bonus if left finger is above the drawer handle and right below
    return is_graspable


def approach_gripper_handle(env: ManagerBasedRLEnv, offset: float = 0.04) -> torch.Tensor:
    """Reward the robot's gripper reaching the drawer handle with the right pose.

    This function returns the distance of fingertips to the handle when the fingers are in a grasping orientation
    (i.e., the left finger is above the handle and the right finger is below the handle). Otherwise, it returns zero.
    """
    # Target object position: (num_envs, 3)
    handle_pos = env.scene["cabinet_frame"].data.target_pos_w[..., 0, :]
    # Fingertips position: (num_envs, n_fingertips, 3)
    ee_fingertips_w = env.scene["ee_frame"].data.target_pos_w[..., 1:, :]
    lfinger_pos = ee_fingertips_w[..., 0, :]
    rfinger_pos = ee_fingertips_w[..., 1, :]

    # Compute the distance of each finger from the handle
    lfinger_dist = torch.abs(lfinger_pos[:, 2] - handle_pos[:, 2])
    rfinger_dist = torch.abs(rfinger_pos[:, 2] - handle_pos[:, 2])

    # Check if hand is in a graspable pose
    is_graspable = (rfinger_pos[:, 2] < handle_pos[:, 2]) & (lfinger_pos[:, 2] > handle_pos[:, 2])

    return is_graspable * ((offset - lfinger_dist) + (offset - rfinger_dist))


def grasp_handle(
    env: ManagerBasedRLEnv, threshold: float, open_joint_pos: float, asset_cfg: SceneEntityCfg
) -> torch.Tensor:
    """Reward for closing the fingers when being close to the handle.

    The :attr:`threshold` is the distance from the handle at which the fingers should be closed.
    The :attr:`open_joint_pos` is the joint position when the fingers are open.

    Note:
        It is assumed that zero joint position corresponds to the fingers being closed.
    """
    ee_tcp_pos = env.scene["ee_frame"].data.target_pos_w[..., 0, :]
    handle_pos = env.scene["cabinet_frame"].data.target_pos_w[..., 0, :]
    gripper_joint_pos = env.scene[asset_cfg.name].data.joint_pos[:, asset_cfg.joint_ids]

    distance = torch.norm(handle_pos - ee_tcp_pos, dim=-1, p=2)
    is_close = distance <= threshold

    return is_close * torch.sum(open_joint_pos - gripper_joint_pos, dim=-1)


def open_drawer_bonus(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """Bonus for opening the drawer given by the joint position of the drawer.

    The bonus is given when the drawer is open. If the grasp is around the handle, the bonus is doubled.
    """
    drawer_pos = env.scene[asset_cfg.name].data.joint_pos[:, asset_cfg.joint_ids[0]]
    is_graspable = align_grasp_around_handle(env).float()

    return (is_graspable + 1.0) * drawer_pos


def multi_stage_open_drawer(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """Multi-stage bonus for opening the drawer.

    Depending on the drawer's position, the reward is given in three stages: easy, medium, and hard.
    This helps the agent to learn to open the drawer in a controlled manner.
    """
    drawer_pos = env.scene[asset_cfg.name].data.joint_pos[:, asset_cfg.joint_ids[0]]
    is_graspable = align_grasp_around_handle(env).float()

    open_easy = (drawer_pos > 0.01) * 0.5
    open_medium = (drawer_pos > 0.2) * is_graspable
    open_hard = (drawer_pos > 0.3) * is_graspable

    return open_easy + open_medium + open_hard


# =============================================================================
# EXAMPLE 2: Pick-and-Place Task Functions (Reference Implementation)
# These show patterns for object manipulation, lifting, and goal tracking
# =============================================================================

def object_is_lifted(
    env: ManagerBasedRLEnv, minimal_height: float, object_cfg: SceneEntityCfg = SceneEntityCfg("object")
) -> torch.Tensor:
    """Reward the agent for lifting the object above the minimal height."""
    object: RigidObject = env.scene[object_cfg.name]
    return torch.where(object.data.root_pos_w[:, 2] > minimal_height, 1.0, 0.0)


def object_ee_distance(
    env: ManagerBasedRLEnv,
    std: float,
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
    ee_frame_cfg: SceneEntityCfg = SceneEntityCfg("ee_frame"),
) -> torch.Tensor:
    """Reward the agent for reaching the object using tanh-kernel."""
    # extract the used quantities (to enable type-hinting)
    object: RigidObject = env.scene[object_cfg.name]
    ee_frame: FrameTransformer = env.scene[ee_frame_cfg.name]
    # Target object position: (num_envs, 3)
    cube_pos_w = object.data.root_pos_w
    # End-effector position: (num_envs, 3)
    ee_w = ee_frame.data.target_pos_w[..., 0, :]
    # Distance of the end-effector to the object: (num_envs,)
    object_ee_distance = torch.norm(cube_pos_w - ee_w, dim=1)
    return 1 - torch.tanh(object_ee_distance / std)


def object_goal_distance(
    env: ManagerBasedRLEnv,
    std: float,
    minimal_height: float,
    command_name: str,
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
) -> torch.Tensor:
    """Reward the agent for tracking the goal pose using tanh-kernel."""
    # extract the used quantities (to enable type-hinting)
    robot: RigidObject = env.scene[robot_cfg.name]
    object: RigidObject = env.scene[object_cfg.name]
    command = env.command_manager.get_command(command_name)
    # compute the desired position in the world frame
    des_pos_b = command[:, :3]
    des_pos_w, *_ = combine_frame_transforms(robot.data.root_pos_w, robot.data.root_quat_w, des_pos_b)
    # distance of the end-effector to the object: (num_envs,)
    distance = torch.norm(des_pos_w - object.data.root_pos_w, dim=1)
    # rewarded if the object is lifted above the threshold
    return (object.data.root_pos_w[:, 2] > minimal_height) * (1 - torch.tanh(distance / std))


# =============================================================================
# TODO: Task-Specific Reward Functions
# Replace the template functions below with your specific task requirements
# =============================================================================

def approach_ee_{TARGET_OBJECT}(env: ManagerBasedRLEnv, threshold: float) -> torch.Tensor:
    """Reward the robot for reaching the {TARGET_OBJECT} using inverse-square law.
    
    TODO: Customize this function for your specific target object.
    
    Args:
        env: The environment instance.
        threshold: Distance threshold for enhanced reward.
        
    Returns:
        Reward tensor of shape (num_envs,).
    """
    ee_tcp_pos = env.scene["ee_frame"].data.target_pos_w[..., 0, :]
    target_pos = env.scene["{TARGET_OBJECT_FRAME}"].data.target_pos_w[..., 0, :]  # TODO: Replace with correct frame name

    # Compute the distance of the end-effector to the target
    distance = torch.norm(target_pos - ee_tcp_pos, dim=-1, p=2)

    # Reward the robot for reaching the target
    reward = 1.0 / (1.0 + distance**2)
    reward = torch.pow(reward, 2)
    return torch.where(distance <= threshold, 2 * reward, reward)


def {TASK_SPECIFIC_ACTION}_success(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
    """Reward for successfully performing {TASK_SPECIFIC_ACTION}.
    
    TODO: Define what constitutes success for your specific task action.
    
    Args:
        env: The environment instance.
        asset_cfg: Configuration for the relevant asset.
        
    Returns:
        Reward tensor of shape (num_envs,).
    """
    # TODO: Implement success criteria for your task
    # Example patterns:
    # - Joint position criteria: asset.data.joint_pos[:, joint_idx] > threshold
    # - Distance criteria: torch.norm(pos_a - pos_b) < threshold  
    # - Boolean success: torch.where(condition, 1.0, 0.0)
    
    # Placeholder implementation
    asset_data = env.scene[asset_cfg.name].data
    # TODO: Replace with actual success logic
    success_condition = torch.ones(env.num_envs, dtype=torch.bool, device=env.device)
    return success_condition.float()


def {MANIPULATION_TARGET}_position_tracking(
    env: ManagerBasedRLEnv,
    std: float,
    target_cfg: SceneEntityCfg = SceneEntityCfg("{MANIPULATION_TARGET}"),
) -> torch.Tensor:
    """Reward for tracking the desired position of {MANIPULATION_TARGET}.
    
    TODO: Customize this for your specific manipulation target.
    
    Args:
        env: The environment instance.
        std: Standard deviation for tanh-kernel.
        target_cfg: Configuration for the manipulation target.
        
    Returns:
        Reward tensor of shape (num_envs,).
    """
    target_object: RigidObject = env.scene[target_cfg.name]
    
    # TODO: Define desired position (could be from command, fixed target, etc.)
    # desired_pos = env.command_manager.get_command("{COMMAND_NAME}")[:, :3]  # From command
    # desired_pos = torch.tensor([x, y, z], device=env.device).expand(env.num_envs, -1)  # Fixed target
    
    # Placeholder: assuming fixed target position
    desired_pos = torch.zeros(env.num_envs, 3, device=env.device)  # TODO: Replace with actual target
    
    # Distance to desired position
    distance = torch.norm(target_object.data.root_pos_w - desired_pos, dim=1)
    return 1 - torch.tanh(distance / std)


# TODO: Configuration Instructions for LLM:
# 
# When populating this template, replace the following placeholders:
# 
# OBJECT/TARGET NAMES:
# - {TARGET_OBJECT}: Main interaction object name (e.g., "handle", "button", "object")
# - {TARGET_OBJECT_FRAME}: Frame name for target object (e.g., "cabinet_frame", "door_frame")
# - {MANIPULATION_TARGET}: Object being manipulated (e.g., "drawer", "door", "object")
# 
# TASK-SPECIFIC ACTIONS:
# - {TASK_SPECIFIC_ACTION}: Main task action (e.g., "open_door", "press_button", "insert_peg")
# 
# REWARD FUNCTION PATTERNS TO USE:
# 1. **Approach Rewards**: Use inverse-square law for smooth approach behavior
# 2. **Alignment Rewards**: Use dot products of rotation matrices for orientation alignment
# 3. **Grasp Rewards**: Check finger positions and joint states for proper grasping
# 4. **Success Rewards**: Binary rewards for task completion
# 5. **Distance Rewards**: Use tanh-kernel for smooth distance-based rewards
# 6. **Multi-stage Rewards**: Progressive rewards for complex tasks
# 
# COMMON REWARD PATTERNS:
# - Distance-based: `1 - torch.tanh(distance / std)` or `1.0 / (1.0 + distance^2)`
# - Binary success: `torch.where(condition, 1.0, 0.0)`
# - Joint position: `asset.data.joint_pos[:, joint_idx]`
# - Orientation alignment: `torch.bmm(vec_a, vec_b)` with rotation matrices
# - Conditional rewards: `condition.float() * reward_value`
# 
# STEPS TO CUSTOMIZE:
# 1. Identify your main interaction objects and their frame names
# 2. Define what constitutes task success
# 3. Choose appropriate reward shaping (approach, alignment, manipulation)
# 4. Use the cabinet and pick-and-place examples as reference patterns
# 5. Remove TODO comments and unused template functions
# 6. Ensure scene entity names match your environment configuration
# 7. Test reward magnitudes to ensure proper learning behavior
</content>
</template_file>

<template_file>
<path>task_name/mdp/terminations.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""Common functions that can be used to activate certain terminations for the {TASK_NAME} task.

The functions can be passed to the :class:`isaaclab.managers.TerminationTermCfg` object to enable
the termination introduced by the function.
"""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

from isaaclab.assets import Articulation, RigidObject
from isaaclab.managers import SceneEntityCfg
from isaaclab.utils.math import combine_frame_transforms

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv


# TODO: Add task-specific termination functions below
# Use the provided examples as templates and customize for your specific task

# =============================================================================
# EXAMPLE 1: Goal-Reaching Termination (Reference Implementation)
# Shows pattern for object reaching target position with command-based goals
# =============================================================================

def object_reached_goal(
    env: ManagerBasedRLEnv,
    command_name: str = "object_pose",
    threshold: float = 0.02,
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    object_cfg: SceneEntityCfg = SceneEntityCfg("object"),
) -> torch.Tensor:
    """Termination condition for the object reaching the goal position.
    
    Args:
        env: The environment.
        command_name: The name of the command that is used to control the object.
        threshold: The threshold for the object to reach the goal position. Defaults to 0.02.
        robot_cfg: The robot configuration. Defaults to SceneEntityCfg("robot").
        object_cfg: The object configuration. Defaults to SceneEntityCfg("object").
        
    Returns:
        Boolean tensor indicating whether each environment should terminate.
    """
    # extract the used quantities (to enable type-hinting)
    robot: RigidObject = env.scene[robot_cfg.name]
    object: RigidObject = env.scene[object_cfg.name]
    command = env.command_manager.get_command(command_name)
    # compute the desired position in the world frame
    des_pos_b = command[:, :3]
    des_pos_w, *_ = combine_frame_transforms(robot.data.root_pos_w, robot.data.root_quat_w, des_pos_b)
    # distance of the end-effector to the object: (num_envs,)
    distance = torch.norm(des_pos_w - object.data.root_pos_w[:, :3], dim=1)
    # rewarded if the object is lifted above the threshold
    return distance < threshold


# =============================================================================
# EXAMPLE 2: Complex Multi-Object Termination (Reference Implementation)  
# Shows pattern for multiple objects with spatial relationships and gripper state
# =============================================================================

def cubes_stacked(
    env: ManagerBasedRLEnv,
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    cube_1_cfg: SceneEntityCfg = SceneEntityCfg("cube_1"),
    cube_2_cfg: SceneEntityCfg = SceneEntityCfg("cube_2"),
    cube_3_cfg: SceneEntityCfg = SceneEntityCfg("cube_3"),
    xy_threshold: float = 0.05,
    height_threshold: float = 0.005,
    height_diff: float = 0.0468,
    gripper_open_val: torch.tensor = torch.tensor([0.04]),
    atol=0.0001,
    rtol=0.0001,
):
    """Termination condition for successfully stacking three cubes.
    
    Checks that cubes are properly aligned in x-y plane and stacked with correct height differences,
    and that the gripper is in the open position.
    
    Args:
        env: The environment.
        robot_cfg: The robot configuration.
        cube_1_cfg: Configuration for the first cube.
        cube_2_cfg: Configuration for the second cube.
        cube_3_cfg: Configuration for the third cube.
        xy_threshold: Maximum allowed x-y distance between cube centers.
        height_threshold: Tolerance for height differences.
        height_diff: Expected height difference between stacked cubes.
        gripper_open_val: Expected gripper joint positions when open.
        atol: Absolute tolerance for gripper position check.
        rtol: Relative tolerance for gripper position check.
        
    Returns:
        Boolean tensor indicating whether each environment should terminate.
    """
    robot: Articulation = env.scene[robot_cfg.name]
    cube_1: RigidObject = env.scene[cube_1_cfg.name]
    cube_2: RigidObject = env.scene[cube_2_cfg.name]
    cube_3: RigidObject = env.scene[cube_3_cfg.name]
    
    pos_diff_c12 = cube_1.data.root_pos_w - cube_2.data.root_pos_w
    pos_diff_c23 = cube_2.data.root_pos_w - cube_3.data.root_pos_w
    
    # Compute cube position difference in x-y plane
    xy_dist_c12 = torch.norm(pos_diff_c12[:, :2], dim=1)
    xy_dist_c23 = torch.norm(pos_diff_c23[:, :2], dim=1)
    
    # Compute cube height difference
    h_dist_c12 = torch.norm(pos_diff_c12[:, 2:], dim=1)
    h_dist_c23 = torch.norm(pos_diff_c23[:, 2:], dim=1)
    
    # Check cube positions
    stacked = torch.logical_and(xy_dist_c12 < xy_threshold, xy_dist_c23 < xy_threshold)
    stacked = torch.logical_and(h_dist_c12 - height_diff < height_threshold, stacked)
    stacked = torch.logical_and(h_dist_c23 - height_diff < height_threshold, stacked)
    
    # Check gripper positions
    stacked = torch.logical_and(
        torch.isclose(robot.data.joint_pos[:, -1], gripper_open_val.to(env.device), atol=atol, rtol=rtol), stacked
    )
    stacked = torch.logical_and(
        torch.isclose(robot.data.joint_pos[:, -2], gripper_open_val.to(env.device), atol=atol, rtol=rtol), stacked
    )
    
    return stacked


# =============================================================================
# TODO: Task-Specific Termination Functions
# Replace the template functions below with your specific task requirements
# =============================================================================

def {MAIN_OBJECTIVE}_completed(
    env: ManagerBasedRLEnv,
    threshold: float = 0.02,
    target_cfg: SceneEntityCfg = SceneEntityCfg("{PRIMARY_OBJECT}"),
) -> torch.Tensor:
    """Termination condition for completing the main objective of {MAIN_OBJECTIVE}.
    
    TODO: Define what constitutes successful completion of your main task objective.
    
    Args:
        env: The environment.
        threshold: Distance/angle threshold for success criteria.
        target_cfg: Configuration for the primary target object.
        
    Returns:
        Boolean tensor indicating whether each environment should terminate.
    """
    target_object: RigidObject = env.scene[target_cfg.name]
    
    # TODO: Implement success criteria for your specific task
    # Common patterns:
    # - Position-based: torch.norm(current_pos - target_pos, dim=1) < threshold
    # - Joint-based: torch.abs(joint_pos - target_joint_pos) < threshold
    # - Height-based: object.data.root_pos_w[:, 2] > min_height
    # - Velocity-based: torch.norm(object.data.root_lin_vel_w, dim=1) < vel_threshold
    
    # Placeholder implementation - replace with actual logic
    success_condition = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)
    
    # Example: Object reached target height
    # success_condition = target_object.data.root_pos_w[:, 2] > threshold
    
    return success_condition


def {SECONDARY_OBJECTIVE}_achieved(
    env: ManagerBasedRLEnv,
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    object_cfg: SceneEntityCfg = SceneEntityCfg("{SECONDARY_OBJECT}"),
    position_threshold: float = 0.05,
    joint_threshold: float = 0.01,
) -> torch.Tensor:
    """Termination condition for achieving {SECONDARY_OBJECTIVE}.
    
    TODO: Define criteria for secondary objectives (e.g., proper grasp, alignment, etc.).
    
    Args:
        env: The environment.
        robot_cfg: The robot configuration.
        object_cfg: Configuration for the secondary object.
        position_threshold: Threshold for position-based criteria.
        joint_threshold: Threshold for joint-based criteria.
        
    Returns:
        Boolean tensor indicating whether each environment should terminate.
    """
    robot: Articulation = env.scene[robot_cfg.name]
    secondary_object: RigidObject = env.scene[object_cfg.name]
    
    # TODO: Implement secondary objective criteria
    # Examples:
    # - Proper grasp: Check gripper joint positions and object proximity
    # - Alignment: Check relative orientation between objects
    # - Stability: Check object velocities are near zero
    # - Contact: Check force/torque sensor readings
    
    # Placeholder - combine multiple conditions with logical_and
    condition_1 = torch.ones(env.num_envs, dtype=torch.bool, device=env.device)  # TODO: Replace
    condition_2 = torch.ones(env.num_envs, dtype=torch.bool, device=env.device)  # TODO: Replace
    
    return torch.logical_and(condition_1, condition_2)


def {MANIPULATION_TARGET}_in_final_state(
    env: ManagerBasedRLEnv,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("{MANIPULATION_TARGET}"),
    final_joint_pos: float = 0.4,
    joint_threshold: float = 0.02,
) -> torch.Tensor:
    """Termination condition for {MANIPULATION_TARGET} reaching its final state.
    
    TODO: Define the final state criteria for articulated objects (e.g., doors, drawers).
    
    Args:
        env: The environment.
        asset_cfg: Configuration for the articulated asset.
        final_joint_pos: Target joint position for the final state.
        joint_threshold: Tolerance for joint position.
        
    Returns:
        Boolean tensor indicating whether each environment should terminate.
    """
    manipulated_asset: Articulation = env.scene[asset_cfg.name]
    
    # TODO: Check joint positions for articulated objects
    # Examples:
    # - Drawer fully open: joint_pos > final_joint_pos - joint_threshold
    # - Door closed: torch.abs(joint_pos - 0.0) < joint_threshold
    # - Valve rotated: torch.abs(joint_pos - target_angle) < angle_threshold
    
    current_joint_pos = manipulated_asset.data.joint_pos[:, asset_cfg.joint_ids[0]]
    
    # Check if joint reached target position
    at_target = torch.abs(current_joint_pos - final_joint_pos) < joint_threshold
    
    return at_target


def task_failed_conditions(
    env: ManagerBasedRLEnv,
    robot_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
    max_ee_height: float = 1.0,
    min_ee_height: float = 0.1,
    max_distance_from_origin: float = 2.0,
) -> torch.Tensor:
    """Termination condition for task failure scenarios.
    
    TODO: Define failure conditions that should terminate the episode early.
    
    Args:
        env: The environment.
        robot_cfg: The robot configuration.
        max_ee_height: Maximum allowed end-effector height.
        min_ee_height: Minimum allowed end-effector height.
        max_distance_from_origin: Maximum allowed distance from origin.
        
    Returns:
        Boolean tensor indicating whether each environment should terminate due to failure.
    """
    # Get end-effector position
    ee_pos = env.scene["ee_frame"].data.target_pos_w[..., 0, :]
    
    # TODO: Define failure conditions
    # Common failure patterns:
    # - End-effector out of bounds
    # - Object dropped/fell
    # - Robot in unsafe configuration
    # - Excessive forces/torques
    # - Time limit exceeded (handled elsewhere usually)
    
    # Check end-effector bounds
    ee_too_high = ee_pos[:, 2] > max_ee_height
    ee_too_low = ee_pos[:, 2] < min_ee_height
    ee_too_far = torch.norm(ee_pos[:, :2], dim=1) > max_distance_from_origin
    
    # Combine failure conditions
    failed = torch.logical_or(ee_too_high, ee_too_low)
    failed = torch.logical_or(failed, ee_too_far)
    
    return failed


# TODO: Configuration Instructions for LLM:
# 
# When populating this template, replace the following placeholders:
# 
# TASK-SPECIFIC NAMES:
# - {TASK_NAME}: Task name for documentation (e.g., "cabinet opening", "pick and place")
# - {MAIN_OBJECTIVE}: Primary task objective (e.g., "drawer_opened", "object_lifted", "door_closed")
# - {SECONDARY_OBJECTIVE}: Secondary objectives (e.g., "proper_grasp", "stable_hold", "aligned_approach")
# - {MANIPULATION_TARGET}: Articulated object being manipulated (e.g., "drawer", "door", "valve")
# 
# OBJECT NAMES:
# - {PRIMARY_OBJECT}: Main object of interaction (e.g., "object", "handle", "button")
# - {SECONDARY_OBJECT}: Additional objects (e.g., "target", "goal", "container")
# 
# TERMINATION PATTERNS TO USE:
# 1. **Success Terminations**: Task completed successfully
#    - Position-based: Object reached target location
#    - State-based: Articulated object in desired configuration
#    - Multi-condition: Multiple criteria must be satisfied
# 
# 2. **Failure Terminations**: Task failed or unsafe conditions
#    - Safety bounds: Robot/objects outside safe regions
#    - Physical constraints: Objects dropped, excessive forces
#    - Time limits: Task taking too long (usually handled by environment)
# 
# 3. **Complex Terminations**: Multiple objects with relationships
#    - Spatial relationships: Objects properly aligned/stacked
#    - Temporal conditions: Sequence of events completed
#    - State combinations: Multiple objects in correct states
# 
# COMMON TERMINATION PATTERNS:
# - Distance check: `torch.norm(pos_a - pos_b, dim=1) < threshold`
# - Joint position: `torch.abs(joint_pos - target_pos) < tolerance`
# - Height check: `object.data.root_pos_w[:, 2] > min_height`
# - Velocity check: `torch.norm(velocity, dim=1) < vel_threshold`
# - Multi-condition: `torch.logical_and(condition_a, condition_b)`
# - Boolean conversion: `condition.float()` for reward compatibility
# 
# STEPS TO CUSTOMIZE:
# 1. Define what constitutes successful task completion
# 2. Identify failure conditions that should end episodes early
# 3. Use appropriate thresholds for your task scale
# 4. Combine conditions with logical operators as needed
# 5. Remove TODO comments and unused template functions
# 6. Test termination conditions to ensure proper episode management
# 7. Consider computational efficiency for real-time performance
</content>
</template_file>

<template_file>
<path>task_name/mdp/__init__.py</path>
<content>
# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
# All rights reserved.
#
# SPDX-License-Identifier: BSD-3-Clause

"""This sub-module contains the functions that are specific to the task name environments."""

from isaaclab.envs.mdp import *  # noqa: F401, F403

from .observations import *  # noqa: F401, F403
from .rewards import *  # noqa: F401, F403
</content>
</template_file>

</template_files>

<output_format>
You must generate complete code for ALL template files using this exact format:

<task_name>your_chosen_task_name</task_name>

<generated_files>
<file>
<path>__init__.py</path>
<content>
[complete file content with all imports and code]
</content>
</file>

<file>
<path>task_name/__init__.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/task_name_env_cfg.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/mdp/__init__.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/mdp/observations.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/mdp/rewards.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/mdp/terminations.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/config/__init__.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/config/franka/__init__.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/config/franka/joint_pos_env_cfg.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/config/franka/agents/__init__.py</path>
<content>
[complete file content]
</content>
</file>

<file>
<path>task_name/config/franka/agents/rsl_rl_ppo_cfg.py</path>
<content>
[complete file content]
</content>
</file>
</generated_files>

<implementation_notes>
- Replace 'task_name' with your chosen task name in all file paths
- Ensure all template placeholders are replaced with actual implementations
- Generate working Isaac Lab code that follows framework conventions
- Create reward functions that guide the robot through logical sub-problems
- Include proper termination conditions for success, failure, and timeouts
- Set reasonable episode lengths based on task complexity
- All objects must be positioned within 0.4m radius of robot base
- Use only imports present in the provided template files
</implementation_notes>
</task_generation>

Now analyze the task description and generate the complete Isaac Lab manipulation task: